[
["introduction.html", "Statistical Inference in People Analytics Chapter 1 Introduction", " Statistical Inference in People Analytics Keith McNulty October 19 2020 Chapter 1 Introduction Lorem Ipsum "],
["the-role-of-inferential-statistics-in-people-analytics.html", "Chapter 2 The Role of Inferential Statistics in People Analytics", " Chapter 2 The Role of Inferential Statistics in People Analytics Lorum ipsum "],
["the-basics-of-the-r-programming-language.html", "Chapter 3 The Basics of the R Programming Language", " Chapter 3 The Basics of the R Programming Language All of the work in this book is implemented in the R statistical programming language which, along with Python, is one of the two languages that I use in my day-to-day statistical analysis. (I intend to release a sister book with the examples coded in Python very soon). I have made efforts to keep the code as simple as possible, and where possible I have tried to avoid the use of too many external packages. For the most part, readers should see (especially in the earlier chapters) that code blocks are short and simple, relying wherever possible on base R functionality. No doubt there are neater and more effective ways to code some of the material in this book using a wide array of R packages, but my priority has been to keep the code simple, consistent and informative. For those who wish to follow the method and theory without the implementations in this book, there is no need to read this chapter. However, the style of this book is to use implementation to illustrate theory and practice, and so tolerance of many code blocks will be necessary. For those who wish to simply replicate my work as quickly as possible, they will be able to avail of the code block copying feature. By pointing their mouse to the top right corner of any input code block, a ‘copy’ icon will appear that will allow them to copy all of the code in that block directly into their session. Assuming all the required external packages have been installed, these code blocks should all be transportable. For those who are extra-inquisitive and want to explore how I constructed graphics used for illustration (for which code is usually not displayed), the best place to go is the Github repository for this book. This chapter is for those who wish to learn the methods in this book but do not know how to use R. However, it is not intended to be a full tutorial on R. There are many more qualified individuals and existing resources that would better serve that purpose - in particular I recommend Hadley Wickham (2016). It is recommended that you consult these resources and become comfortable with the basics of R before proceeding into the later chapters of this book. However, acknowledging that many will want to dive in sooner rather than later, this chapter covers the absolute basics of R that will allow the uninitiated reader to proceed with at least some orientation. References "],
["what-is-r.html", "3.1 What is R?", " 3.1 What is R? R is a programming language that was originally developed by and for statisticians, but has in recent years expanded greatly in its capabilities and the environments in which it is used, with extensive use nowadays in academia and the public and private sectors. There are many advantages to using a programming language like R. Here are some. It is completely free and open source. It is faster and more efficient with memory than popular graphical user interface analytics tools It facilitates easier replication of analysis from person to person compared with many alternatives It has a large and growing global community of active users It has a large and rapidly growing universe of packages which are all free and which provide the ability to do an extremely wide range of general and highly specialized tasks - both statistical and otherwise. There is often heated debate about which languages/tools are better for doing non-trivial statistical analysis. I personally find that R and Python far outperform the other options at this point in time, but that is just my opinion based on my experience, and there are no doubt many others that will disagree. "],
["how-to-start-using-r.html", "3.2 How to start using R", " 3.2 How to start using R Just like most programming languages, R itself is an interpreter which receives input and returns output. It is not very easy to use without an IDE. An IDE is an Integrated Development Environment, which is a convenient user interface allowing an R programmer to do all their main tasks including writing and running R code, saving files, viewing data and plots, integrating code into documents and many other things. By far the most popular IDE for R is RStudio. An example of what the RStudio IDE looks like can be seen in Figure 3.1. Figure 3.1: The RStudio IDE To start using R, follow these steps: Download and install the latest version of R from https://www.r-project.org/. Ensure that the version suits your operating system. Download the latest version of the RStudio IDE from https://rstudio.com/products/rstudio/ and view the video on that page to familiarize yourself with its features. Open RStudio and play around. The initial stages of using R can be challenging, mostly due to the need to become familiar with how R understands, stores and processes data. Extensive trial and error is a learning necessity. Perseverance is important in these early stages, as well as an openness to seek help from others either in person or via online forums. "],
["data-in-r.html", "3.3 Data in R", " 3.3 Data in R As you start to do tasks involving data in R, you will generally want to store the things you create so that you can refer to them later. Simply calculating something does not store it in R. For example, a simple calculation like this can be performed easily: 3 + 3 ## [1] 6 However, as soon as the calculation is complete, it is forgotten by R because the result hasn’t been assigned anywhere. To store something in your R session, you will assign it a name using the &lt;- command. So I can assign my previous calculation to an object called sum, and this allows me to access the answer at any time. # store the result sum &lt;- 3 + 3 # now I can work with it sum + 3 ## [1] 9 You will see above that you can comment your code by simply adding a # to the start of a line to ensure that the line is ignored by the interpreter. Note that assignment to an object does not result in the value being displayed. To display the value, the name of the object must be typed, the print() command used or the command should be wrapped in parentheses. # show me the value of sum sum ## [1] 6 # assign sum + 3 to new_sum and show the result (new_sum &lt;- sum + 3) ## [1] 9 3.3.1 Data types All data in R has an associated type, to reflect the wide range of data that R is able to work with. The typeof() function can be used to see the type of a single scalar value. Let’s look at the most common scalar data types. Numeric data can be in integer form or double (decimal) form my_integer &lt;- 1L #integers can be signified by adding an &#39;L&#39; to the end my_double &lt;- 6.38 typeof(my_integer) ## [1] &quot;integer&quot; typeof(my_double) ## [1] &quot;double&quot; Character data is text data surrounded by single or double quotes my_character &lt;- &quot;THIS IS TEXT&quot; typeof(my_character) ## [1] &quot;character&quot; Logical data takes the form TRUE or FALSE: my_logical &lt;- TRUE typeof(my_logical) ## [1] &quot;logical&quot; 3.3.2 Homogeneous data structures Vectors are one dimensional structures containing data of the same type and are notated by using c(). The type of the vector can be viewed using the str() function: my_double_vector &lt;- c(2.3, 6.8, 4.5, 65, 6) print(my_double_vector) ## [1] 2.3 6.8 4.5 65.0 6.0 str(my_double_vector) ## num [1:5] 2.3 6.8 4.5 65 6 Categorical data - which takes only a finite number of possible values, can be stored as a factor vector to make it easier to perform grouping and manipulation: categories &lt;- factor(c(&quot;Category A&quot;, &quot;Category B&quot;, &quot;Category C&quot;, &quot;Category A&quot;, &quot;Category C&quot;)) str(categories) ## Factor w/ 3 levels &quot;Category A&quot;,&quot;Category B&quot;,..: 1 2 3 1 3 If needed, the factors can be given order: # character vector ranking &lt;- c(&quot;Medium&quot;, &quot;High&quot;, &quot;Low&quot;) str(ranking) ## chr [1:3] &quot;Medium&quot; &quot;High&quot; &quot;Low&quot; # turn it into an ordered factor ranking_factors &lt;- ordered(ranking, levels = c(&quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;)) str(ranking_factors) ## Ord.factor w/ 3 levels &quot;Low&quot;&lt;&quot;Medium&quot;&lt;..: 2 3 1 The number of elements in a vector can be seen using the length() function: length(categories) ## [1] 5 Simple numeric sequence vectors can be created using shorthand notation: (my_sequence &lt;- 1:10) ## [1] 1 2 3 4 5 6 7 8 9 10 If you try to mix data types inside a vector, it will usually result in type coercion, where one of the types is forced into a different type. Often this means the vector will become a character vector: # numeric sequence vector vec &lt;- 1:5 str(vec) ## int [1:5] 1 2 3 4 5 # create a new vector containing vec and the character &quot;hello&quot; new_vec &lt;- c(vec, &quot;hello&quot;) # numeric values have been coerced into their character equivalents str(new_vec) ## chr [1:6] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;hello&quot; But sometimes logical or factor will be coerced to numeric: # attempt a mixed logical and numeric mix &lt;- c(TRUE, 6) # logical has been converted to binary numeric (TRUE = 1) str(mix) ## num [1:2] 1 6 # try to add a numeric to our previous categories factor vector new_categories &lt;- c(categories, 1) # categories have been coerced to their background integer representations str(new_categories) ## num [1:6] 1 2 3 1 3 1 Matrices are two-dimensional data structures of the same type, and are built from a vector by defining the number of rows and columns. Data is read into the matrix down the columns, one at a time. Matrices are rarely used for non-numeric data types. # create a 2x2 with the first four integers (m &lt;- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2)) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 Arrays are n-dimensional data structures with the same data type, and are not used extensively by most R users. 3.3.3 Heterogeneous data structures Lists are one dimensional data structures that can take data of any type: my_list &lt;- list(6, TRUE, &quot;hello&quot;) str(my_list) ## List of 3 ## $ : num 6 ## $ : logi TRUE ## $ : chr &quot;hello&quot; Lists can contain any data of any dimension, each element can be given a name: new_list &lt;- list(scalar = 6, vector = c(&quot;Hello&quot;, &quot;Goodbye&quot;), matrix = matrix(1:4, nrow = 2, ncol = 2)) str(new_list) ## List of 3 ## $ scalar: num 6 ## $ vector: chr [1:2] &quot;Hello&quot; &quot;Goodbye&quot; ## $ matrix: int [1:2, 1:2] 1 2 3 4 Named list elements can be accessed by using $: new_list$matrix ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 Dataframes are the most used data structure in R, and are effectively a named list of vectors of the same length, with each vector as a column. As such, it has a great resemblance to a typical database table or spreadsheet. # two vectors of different types but same length names &lt;- c(&quot;John&quot;, &quot;Ayesha&quot;) ages &lt;- c(31, 24) # create a dataframe (df &lt;- data.frame(names, ages)) ## names ages ## 1 John 31 ## 2 Ayesha 24 #get types of columns str(df) ## &#39;data.frame&#39;: 2 obs. of 2 variables: ## $ names: chr &quot;John&quot; &quot;Ayesha&quot; ## $ ages : num 31 24 #get dimensions of df dim(df) ## [1] 2 2 "],
["working-with-dataframes.html", "3.4 Working with dataframes", " 3.4 Working with dataframes The dataframe is the most common data structure used by analysts in R, due to its similarity to data tables found in databases and spreadsheets. We will work almost entirely with datafames in this book, so let’s get to know them. 3.4.1 Loading and tidying data in dataframes To work with data in R, you usually need to pull it in from an outside source into a dataframe1. R facilitates numerous ways of importing data from simple csv files, from Excel files, from online sources or from databases. In this book we will be loading all our datasets from online csv files, and the read.csv() function makes this easy. Let’s load a data set that we will be using later - the salespeople data set, which contains some information on the sales, average customer ratings and performance ratings of salespeople. The read.csv() function can accept a URL address of the file if it is online. # url of data set url &lt;- &quot;https://raw.githubusercontent.com/keithmcnulty/eampa/master/data/salespeople.csv&quot; # load the dataset and store it as a dataframe called salespeople salespeople &lt;- read.csv(url) We might not want to display this entire dataset before knowing how big it is. We can view the dimensions and if it is too big to display, we can use the head() function to display just the first few rows. dim(salespeople) ## [1] 351 4 # view first few rows head(salespeople) ## promoted sales customer_rate performance ## 1 0 594 3.94 2 ## 2 0 446 4.06 3 ## 3 1 674 3.83 4 ## 4 0 525 3.62 2 ## 5 1 657 4.40 3 ## 6 1 918 4.54 2 We can view a specific column by using $, and we can use square brackets to view a specific entry. For example if we wanted to see the 6th entry of the sales column: salespeople$sales[6] ## [1] 918 Alternatively we can use a [row, column] index to get a specific entry in the dataframe. salespeople[34, 4] ## [1] 3 We can take a look at the data types using str() str(salespeople) ## &#39;data.frame&#39;: 351 obs. of 4 variables: ## $ promoted : int 0 0 1 0 1 1 0 0 0 0 ... ## $ sales : int 594 446 674 525 657 918 318 364 342 387 ... ## $ customer_rate: num 3.94 4.06 3.83 3.62 4.4 4.54 3.09 4.89 3.74 3 ... ## $ performance : int 2 3 4 2 3 2 3 1 3 3 ... We can also see a statistical summary of each column using summary(), which tells us various statistics depending on the type of the column. summary(salespeople) ## promoted sales customer_rate performance ## Min. :0.0000 Min. :151.0 Min. :1.000 Min. :1.0 ## 1st Qu.:0.0000 1st Qu.:389.2 1st Qu.:3.000 1st Qu.:2.0 ## Median :0.0000 Median :475.0 Median :3.620 Median :3.0 ## Mean :0.3219 Mean :527.0 Mean :3.608 Mean :2.5 ## 3rd Qu.:1.0000 3rd Qu.:667.2 3rd Qu.:4.290 3rd Qu.:3.0 ## Max. :1.0000 Max. :945.0 Max. :5.000 Max. :4.0 ## NA&#39;s :1 NA&#39;s :1 NA&#39;s :1 Note that there is missing data in this dataframe, indicated by NAs in the summary. Missing data is identified by a special NA value in R. This should not be confused with \"NA\", which is simply a character string. The function is.na() will look at all values in a vector or dataframe and return TRUE or FALSE based on whether they are NA or not. By adding these up using the sum() function, it will take TRUE as 1 and FALSE as 0, which effectively provides a count of missing data. sum(is.na(salespeople)) ## [1] 3 This is a small number of NAs given the dimensions of our data set and we might want to remove the rows of data that contain NAs. The easiest way is to use the complete.cases() function, which identifies the rows that have no NAs, and then we can select those rows from the dataframe based on that condition. Note that you can overwrite objects with the same name in R. salespeople &lt;- salespeople[complete.cases(salespeople), ] #confirm no NAs sum(is.na(salespeople)) ## [1] 0 We can see the unique values of a vector or column using the unique() function. unique(salespeople$performance) ## [1] 2 3 4 1 If we need to change the type of a column in a dataframe, we can use the as.numeric(), as.character(), as.logical() or as.factor() functions. For example, given that there are only four unique values for the performance column, we may want to convert it to a factor. salespeople$performance &lt;- as.factor(salespeople$performance) str(salespeople) ## &#39;data.frame&#39;: 350 obs. of 4 variables: ## $ promoted : int 0 0 1 0 1 1 0 0 0 0 ... ## $ sales : int 594 446 674 525 657 918 318 364 342 387 ... ## $ customer_rate: num 3.94 4.06 3.83 3.62 4.4 4.54 3.09 4.89 3.74 3 ... ## $ performance : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 2 3 4 2 3 2 3 1 3 3 ... 3.4.2 Manipulating dataframes Dataframes can be subsetted to contain only rows that satisfy specific conditions: sales_720 &lt;- subset(salespeople, subset = sales == 720) Note the use of == which is used in many programming languages to test for precise equality. Similarly we can select columns based on inequalities (&gt; for ‘greater than’, &lt; for ‘less than’, &gt;= for ‘greater than or equal to’, &lt;= for ‘less than or equal to’, or != for ‘not equal to’). For example: high_sales &lt;- subset(salespeople, subset = sales &gt;= 700) head(high_sales) ## promoted sales customer_rate performance ## 6 1 918 4.54 2 ## 12 1 716 3.16 3 ## 20 1 937 5.00 2 ## 21 1 702 3.53 4 ## 25 1 819 4.45 2 ## 26 1 736 3.94 4 To select specific columns use the select argument: salespeople_sales_perf &lt;- subset(salespeople, select = c(&quot;sales&quot;, &quot;performance&quot;)) head(salespeople_sales_perf) ## sales performance ## 1 594 2 ## 2 446 3 ## 3 674 4 ## 4 525 2 ## 5 657 3 ## 6 918 2 Two dataframes with the same column names can be combined by their rows: low_sales &lt;- subset(salespeople, subset = sales &lt; 400) # bind the rows of low_sales and high_sales together low_and_high_sales = rbind(low_sales, high_sales) head(low_and_high_sales) ## promoted sales customer_rate performance ## 7 0 318 3.09 3 ## 8 0 364 4.89 1 ## 9 0 342 3.74 3 ## 10 0 387 3.00 3 ## 15 0 344 3.02 2 ## 16 0 372 3.87 3 Two dataframes with different column names can be combined by their columns: # two dataframes with two columns each sales_perf &lt;- subset(salespeople, select = c(&quot;sales&quot;, &quot;performance&quot;)) prom_custrate &lt;- subset(salespeople, select = c(&quot;promoted&quot;, &quot;customer_rate&quot;)) # bind the columns to create a dataframe with four columns full_df &lt;- cbind(sales_perf, prom_custrate) head(full_df) ## sales performance promoted customer_rate ## 1 594 2 0 3.94 ## 2 446 3 0 4.06 ## 3 674 4 1 3.83 ## 4 525 2 0 3.62 ## 5 657 3 1 4.40 ## 6 918 2 1 4.54 R also has some inbuilt data sets which are for testing and playing with, for example check out mtcars by typing it into the terminal, or type data() to see a full list of in built data sets↩︎ "],
["functions-packages-and-libraries.html", "3.5 Functions, packages and libraries", " 3.5 Functions, packages and libraries In the code so far we have used a variety of functions. For example head(), subset(), rbind(). Functions are operations that take certain defined inputs and return an output. Functions exist to perform common useful operations. 3.5.1 Using functions Functions usually take one or more arguments. Often there are a large number of arguments that a function can take, but many are optional and not required to be specified by the user, For example, the function head(), which displays the first rows of a dataframe2, has only one required argument x: the name of the dataframe. A second argument is optional, n: the number of rows to display. If n is not entered, it is assumed to have the default value n = 6. When running a function, you can either specify the arguments by name or you can enter them in order without their names. If you enter then without their names, R expects the arguments to be entered in exactly the right order. # see the head of salespeople, with the default of six rows head(salespeople) ## promoted sales customer_rate performance ## 1 0 594 3.94 2 ## 2 0 446 4.06 3 ## 3 1 674 3.83 4 ## 4 0 525 3.62 2 ## 5 1 657 4.40 3 ## 6 1 918 4.54 2 # see fewer rows - arguments need to be in the right order if not named head(salespeople, 3) ## promoted sales customer_rate performance ## 1 0 594 3.94 2 ## 2 0 446 4.06 3 ## 3 1 674 3.83 4 # or if you don&#39;t know the right order, name your arguments and you can put them in any order head(n = 3, x = salespeople) ## promoted sales customer_rate performance ## 1 0 594 3.94 2 ## 2 0 446 4.06 3 ## 3 1 674 3.83 4 3.5.2 Help with functions Most functions in R have excellent help documentation. To get help on the head() function, type help(head) or ?head. This will display the results in the Help browser window in RStudio. Alternatively you can open the Help browser window directly in RStudio and do a search there. An example of the browser results for head() is in Figure 3.2. Figure 3.2: Results of a search for the head() function in the RStudio Help browser The help page normally shows the following: Description of the purpose of the function Usage examples, so you can quickly see how it is used Arguments list so you can see the names and order of arguments Details or notes on further considerations on use Expected value of the output (for example head() is expected to return a similar object to its first input x) Examples to help orient you further (sometimes examples can be very abstract in nature and not so helpful to users) 3.5.3 Installing packages All the functions that we have used so far exist in the base R installation. However, the beauty of open source languages like R is that users can write their own functions or resources and release them to others via packages. A package is an additional module that can be installed easily and which makes resources available which are not in the base R installation. In this book we will be using functions from both base R and from popular and useful packages. As an example, a popular package used for statistical modelling is the MASS package, which is based on methods in a popular applied statistics book3. Before an external package can be used, it must be installed into your package library using install.packages(). So to install MASS, type install.packages(\"MASS\") into the console. This will send R to the main internet repository for R packages (known as CRAN), it will find the right version of MASS for your operating system and download and install it into your package library. If MASS needs other packages in order to work, it will also install these packages. If you want to install more than one package, put the names of the packages inside a character vector - for example: my_packages &lt;- c(&quot;MASS&quot;, &quot;DescTools&quot;, &quot;dplyr&quot;) install.packages(my_packages) Once you have installed a package, you can see what functions are available by calling for help on it, for example using help(package = MASS). 3.5.4 Using packages Once you have installed a package into your package library, to use it in your R session you need to load it using the library() function. For example, to load MASS after installing it, use library(MASS). Often nothing will happen when you use this command, but rest assured the package has been loaded and you can start to use the functions inside it. Sometimes when you load the package a series of messages will display, usually to make you aware of certain things that you need to keep in mind when using the package. Note that whenever you see the library() command in this book, it is assumed that you have already installed the package in that command. If you have not, the library() command will fail. Once a package is loaded from your library, you can use any of the functions inside it. For example, the stepAIC() function is not available before you load the MASS package but becomes available after it is loaded. In this sense, functions ‘belong’ to packages. Problems can occur when you load packages that contain functions with the same name as functions that already exist in your R session. Often the messages you see when loading a package will alert you to this. When R is faced with a situation where a function exists in multiple packages you have loaded, R always defaults to the function in the most recently loaded package. This may not always be what you intended. One way to completely avoid this issue is to get in the habit of namespacing your functions. To namespace, you simply use package::function(), so to safely call stepAIC() from MASS, you use MASS::stepAIC(). At all times in this book when a function is being called from a package outside base R, I use namespacing to call that function. This should help avoid confusion about which packages are being used for which functions. 3.5.5 The pipe operator Even in the most elementary briefing about R, it is very difficult to ignore the pipe operator. The pipe operator makes code more natural to read and write and reduces the typical computing problem of many nested operations inside parentheses. The pipe operator comes inside many R packages, particularly magrittr and dplyr. As an example, imagine we wanted to do the following two operations in one command: Subset salespeople to only the sales values of those with sales less than 500 Take the mean of those values In base R, one way to do this is: mean(subset(salespeople$sales, subset = salespeople$sales &lt; 500)) ## [1] 388.6684 This is nested, and needs to be read from the inside out in order to align with the instructions. The pipe operator %&gt;% takes the command that comes before it and places it inside the function that follows it (by default as the first argument). This reduces complexity and allows you to follow the logic more clearly: # load magrittr library to get the pipe operator library(magrittr) # use the pipe operator to lay out the steps more logically subset(salespeople$sales, subset = salespeople$sales &lt; 500) %&gt;% mean() ## [1] 388.6684 This can be extended to perform arbitrarily many operations in one piped command: subset(salespeople$sales, subset = salespeople$sales &lt; 500) %&gt;% # get the subsetted data mean() %&gt;% # take the mean value round() # round to the nearest integer ## [1] 389 The pipe operator is unique to R and is very widely used because it helps to make code more readable, it reduces complexity, and it helps orient around a common “grammar” for the manipulation of data. The pipe operator helps you to structure your code more clearly around nouns (objects), verbs (functions) and adverbs (arguments of functions). One of the most developed sets of packages in R that follows these principles is the tidyverse family of packages, which I encourage you to explore. It actually has a broader definition but is mostly used for showing the first rows of a dataframe↩︎ W. N. Venables (2002)↩︎ "],
["errors-warnings-and-messages.html", "3.6 Errors, warnings and messages", " 3.6 Errors, warnings and messages As I mentioned earlier in this chapter, getting familiar with R can be frustrating at the beginning if you have never programmed before. You can expect to regularly see messages, warnings or errors in response to your commands. I encourage you to regard these as your friend rather than your enemy. It is very tempting to take the latter approach when you are starting out, but over time I hope you will appreciate some wisdom from my words. Errors are serious problems which usually result in the halting of your code and a failure to return your requested output. They usually come with an indication of the source of the error and these can sometimes be easy to understand and sometimes frustratingly vague and abstract. For example, an easy to understand error is: subset(salespeople, subset = sales = 720) Error: unexpected &#39;=&#39; in &quot;subset(salespeople, subset = sales =&quot; This helps you see that you have used sales = 720 as a condition to subset your data, when you should have used sales == 720 for precise equality. A much more challenging error to understand is: head[salespeople] Error in head[salespeople] : object of type &#39;closure&#39; is not subsettable When first faced with an error that you can’t understand, try not to get frustrated and proceed in the knowledge that it usually can be fixed easily and quickly. Often the problem is much more obvious than you think, and if not, there is still a 99% likelihood that others have made this error and you can read about it online. The first step is to take a look at your code to see if you can spot what you did wrong. In this case you may see that you have used square brackets [] instead of parentheses() when calling your head() function. If you cannot see what is wrong, the next step is to ask a colleague or do an internet search with the text of the error message you receive, or to consult online forums like https://stackoverflow.com. The more experienced you become, the easier it is to interpret error messages. Warnings are less serious and usually alert you to something that you might be overlooking and which could indicate a problem with the output. In many cases you can ignore warnings, but sometimes they are an important reminder to go back and edit your code. For example, you may run a model which doesn’t converge, and while this does not stop R from returning results, it is also very useful for you to know that it didn’t converge. Messages are pieces of information that may or may not be useful to you at a particular point in time. Sometimes you will receive messages when you load a package from your library. Sometimes messages will keep you up to date on the progress of a process that is taking a long time to execute. "],
["learning-exercises.html", "3.7 Learning exercises", " 3.7 Learning exercises 3.7.1 Discussion questions Describe the following data types: numeric, character, logical, factor. Why is a vector known as a homogeneous data structure? Give an example of a heterogeneous data structure in R. What is the difference between NA and \"NA\"? What operator is used to see named elements of a list and named columns of a dataframe? Describe some functions that are used to manipulate dataframes. What is a package and how do you install and use a new package? Describe what is meant by ‘namespacing’ and why it might be useful? What is the pipe operator and why is it popular in R? What is the difference between an error and a warning in R? 3.7.2 Data exercises Create a character vector called my_names that contains all your first, middle and last names as elements. Calculate the length of my_names. Create a second numeric vector called which which corresponds to my_names. The entries should be the position of each name in the order of your full name. Verify that it has the same length as my_names. Create a dataframe called names which consists of the two vectors my_names and which as columns. Calculate the dimensions of names. Create a new dataframe new_names with the which column converted to character type. Verify that your command worked using str(). Use read.csv() to load the dataset of student test data found at https://raw.githubusercontent.com/keithmcnulty/eampa/master/data/ugtests.csv using the name ugtests. Calculate the dimensions of ugtests and view the first three rows only. View a statistical summary of all of the columns of ugtests. Determine if there are any missing values. View the subset of ugtests for values of Yr1 greater than 50. Install and load the package dplyr. Look up the help for the filter() function in this package and try to use it to repeat the task in the previous question. Using the pipe operator, write code to find the mean of the Yr1 test scores for all those who achieved Yr3 test scores greater than 100. Round this mean to the nearest integer. If you did not use filter() from dplyr in the previous question, rewrite your answer using filter(), being sure to namespace where necessary. "],
["statistics-foundations.html", "Chapter 4 Statistics foundations", " Chapter 4 Statistics foundations To properly understand multivariate models, an analyst needs to have a decent grasp of foundational statistics. Many of the assumptions and results of multivariate models require an understanding of these foundations in order to be properly interpreted. There are three topics that are particularly important for those proceeding further in this book: Descriptive statistics of populations and samples Distribution of random variables Hypothesis testing If you have never really studied these topics, I would strongly recommend taking a course in them and spending good time getting to know them. Again, just as the last chapter was not intended to be a comprehensive tutorial on R, neither is this chapter intended to be a comprehensive tutorial on introductory statistics. However, we will introduce some key concepts here that are critical to understanding later chapters, and as always we will illustrate using real data examples in R. In preparation for this chapter we are going to download a data set that we will work through in a later chapter, and use it for practical examples and illustration purposes. The data are a set of information on the sales, customer ratings and performance ratings on a set of 351 salespeople as well as an indication of whether or not they were promoted. # use online url to download salespeople data url &lt;- &quot;https://raw.githubusercontent.com/keithmcnulty/eampa/master/data/salespeople.csv&quot; salespeople &lt;- read.csv(url) Let’s take a brief look at first few rows of this data to make sure we know what is inside it. head(salespeople) ## promoted sales customer_rate performance ## 1 0 594 3.94 2 ## 2 0 446 4.06 3 ## 3 1 674 3.83 4 ## 4 0 525 3.62 2 ## 5 1 657 4.40 3 ## 6 1 918 4.54 2 And let’s understand the structure of this data. str(salespeople) ## &#39;data.frame&#39;: 351 obs. of 4 variables: ## $ promoted : int 0 0 1 0 1 1 0 0 0 0 ... ## $ sales : int 594 446 674 525 657 918 318 364 342 387 ... ## $ customer_rate: num 3.94 4.06 3.83 3.62 4.4 4.54 3.09 4.89 3.74 3 ... ## $ performance : int 2 3 4 2 3 2 3 1 3 3 ... It looks like: promoted is a binary value, either 1 or 0. Presumably 1 means ‘was promoted’ and 0 means ‘was not promoted’. sales and customer_rate look like normal numerical values. performance looks like a set of performance categories - there appear to be four based on what we can see. "],
["elementary-descriptive-statistics-of-populations-and-samples.html", "4.1 Elementary descriptive statistics of populations and samples", " 4.1 Elementary descriptive statistics of populations and samples Any collection of numerical data on a given measure will have a mean and a variance around that mean. 4.1.1 Mean, variance and standard deviation The mean is the average value of the data and is defined by adding up all the values and dividing by the number of datapoints. If the data set contains the \\(N\\) datapoints \\(x_1, x_2, ..., x_N\\), then the mean is defined as: \\[ \\frac{1}{N}\\sum_{i = 1}^{N}x_i \\] The mean can have a different interpretation depending on the type of data being studied. Let’s look at the mean of three different columns of our salespeople data, making sure to ignore any missing data. mean(salespeople$sales, na.rm = TRUE) ## [1] 527.0057 This looks very intuitive and appears to be the average amount of sales made by the individuals in the data set. mean(salespeople$promoted, na.rm = TRUE) ## [1] 0.3219373 Given that this data can only have the value of 0 or 1, we interpret this mean as likelihood or expectation that an individual will be labelled as 1. That is, the average probability of promotion in the data set. If this data showed a perfectly even likelihood of promotion, we would expect this to take the value of 0.5. But it is lower than 0.5, which tells us that the majority of individuals are not promoted. mean(salespeople$performance, na.rm = TRUE) ## [1] 2.5 Given that this data can only have the values 1, 2, 3 or 4, we interpret this as the expected value of the performance rating in the dataset. Higher or lower means inform us about the distribution of the performance ratings. A low mean will indicate a skew towards a low rating, and a high mean will indicate a skew towards a high rating. Other common statistical summary measures include the median, which is the middle value when the values are ranked in order, and the mode, which is the most frequently occurring value. The variance is a measure of how much the data varies around its mean. There are two different definitions of variance. The population variance assumes that there is no sampling and is defined as the average squared difference from the mean. If \\(\\mu\\) is the mean value of \\(X = x_1, x_2, ..., x_N\\), then population variance is \\[ \\mathrm{Var}_p(X) = \\frac{1}{N}\\sum_{i = 1}^{N}(x_i - \\mu)^2 \\] The sample variance assumes there is sampling and attempts to estimate the variance of a larger population by applying Bessel’s Correction to account for potential sampling error. The sample variance is: \\[ \\mathrm{Var}_s(X) = \\frac{1}{N-1}\\sum_{i = 1}^{N}(x_i - \\mu)^2 \\] You can see that, on the same dataset, \\[ \\mathrm{Var}_p(X) = \\frac{N - 1}{N}\\mathrm{Var}_s(X) \\] so as the dataset gets larger, the sample variance and the population variance become less and less distinguishable, which intuitively makes sense. Note that in R and in many other statistical software packages, the sample variance is calculated by default: # sample variance (sample_variance_sales &lt;- var(salespeople$sales, na.rm = TRUE)) ## [1] 34308.11 So where necessary, we need to apply a transformation to get the population variance: # population variance (need length of non-NA data) n &lt;- length(!is.na(salespeople$sales)) (population_variance_sales &lt;- ((n-1)/n) * sample_variance_sales) ## [1] 34210.37 Variance does not have intuitive scale relative to the data being studied, because we have used a ‘squared distance metric’, therefore we can square-root it to get a measure of ‘deviance’ on the same scale as the data. We call this the standard deviation \\(\\sigma\\), where \\(\\mathrm{Var}(X) = \\sigma^2\\). As with variance, standard deviation has both population and sample versions, and the sample version is calculated by default. Conversion between the two takes the form \\[ \\sigma_p = \\sqrt{\\frac{N-1}{N}}\\sigma_s \\] # sample standard deviation (sample_sd_sales &lt;- sd(salespeople$sales, na.rm = TRUE)) ## [1] 185.2245 # verify that sample sd is sqrt(sample var) sd(salespeople$sales, na.rm = TRUE) == sqrt(sample_variance_sales) ## [1] TRUE # calculate population standard deviation (population_sd_sales &lt;- sqrt((n-1)/n) * sample_sd_sales) ## [1] 184.9605 Given the range of sales is [151, 945] and the mean is 527, we see that the standard deviation gives a more intuitive sense of the ‘spread’ of the data relative to its inherent scale. 4.1.2 Covariance and correlation The covariance between two variables is a measure the extent to which one changes as the other changes. If \\(Y = y_1, y_2, ..., y_N\\) is a second variable, and \\(\\mu_X\\) and \\(\\mu_Y\\) are the means of X and Y respectively, then the sample covariance of \\(X\\) and \\(Y\\) is defined as: \\[ \\mathrm{cov}_s(X, Y) = \\frac{1}{N - 1}\\sum_{i = 1}^{N}(x_i - \\mu_X)(y_i - \\mu_Y) \\] and as with variance, the population covariance is \\[ \\mathrm{cov}_p(X, Y) = \\frac{N-1}{N}\\mathrm{cov}_s(X, Y) \\] Again, the sample covariance is the default in R and we need to transform to obtain the population covariance: # get sample covariance for sales and customer_rate, ignoring observations with missing data (sample_cov &lt;- cov(salespeople$sales, salespeople$customer_rate, use = &quot;complete.obs&quot;)) ## [1] 55.81769 # convert to population covariance (need number of complete obs) cols &lt;- subset(salespeople, select = c(&quot;sales&quot;, &quot;customer_rate&quot;)) N &lt;- nrow(cols[complete.cases(cols), ]) (population_cov &lt;- ((N-1)/N) * sample_cov) ## [1] 55.65821 As can be seen, the difference in covariance is very small between the sample and population versions, and both confirm a positive relationship between sales and customer rating. However, we again see this issue that there is no intuitive sense of scale for this measure. Pearson’s correlation coefficient divides the covariance by the product of the standard deviations of the two variables.. \\[ r_{X, Y} = \\frac{\\mathrm{cov}(X, Y)}{\\sigma_X\\sigma_Y} \\] This creates a maximum scale of -1 to 1 for \\(r_{X, Y}\\) which is an intuitive way of understanding both the direction and scale of the relationship between X and Y, with -1 indicating that X increases perfectly as Y decreases, 1 indicating that X increases perfectly as X increases, and 0 indicating that there is no relationship between the two. As before, there is a sample and population version of the correlation coefficient and R calculates the sample version by default. Similar transformations can be used to determine a population correlation coefficient and over large populations the two measures converge. # calculate sample correlation between sales and customer_rate cor(salespeople$sales, salespeople$customer_rate, use = &quot;complete.obs&quot;) ## [1] 0.337805 This tells us that there is a moderate positive correlation between sales and customer_rating. You will notice that we have so far used two variables on a continuous scale to demonstrate covariance and correlation. Pearson’s correlation can also be used between a continuous scale and a dichotomous (binary) scale variable, and this is known as a point-biserial correlation. cor(salespeople$sales, salespeople$promoted, use = &quot;complete.obs&quot;) ## [1] 0.8511283 Correlating ranked variables involve an adjusted approach leading to Spearman’s correlation coefficient or Kendall’s Tau, among others. We will not dive into the mathematics of this here, but a good source is P. K. Bhattacharya (2016). Spearman’s or Kendall’s variant should be used whenever at least one of the variables is a ranked variable, and both variants are available in R. # spearman&#39;s correlation cor(salespeople$sales, salespeople$performance, method = &quot;spearman&quot;, use = &quot;complete.obs&quot;) ## [1] 0.2735446 # kendall&#39;s correlation cor(salespeople$sales, salespeople$performance, method = &quot;kendall&quot;, use = &quot;complete.obs&quot;) ## [1] 0.2073609 Both in this case indicate a low to moderate correlation. Spearman or Kendall can also be used to correlate a ranked and a dichotomous variable, and this is known as a rank-biserial correlation. References "],
["distribution-of-random-variables.html", "4.2 Distribution of random variables", " 4.2 Distribution of random variables In our last chapter we discussed how, when we build a model, we are using a set of sample data to infer a general relationship on a bigger population. A major underlying assumption in our inference is that we believe the real life variables we are dealing with are random in nature. For example, we might be trying to model the drivers of the voting choice of millions of people in a national election, but we may only have sample data on a few thousand people. When we infer nationwide voting intentions from our sample, we assume that the characteristics of the voting population are random variables. 4.2.1 Sampling of random variables When we describe variables as random, we are assuming that they take a form which is independent and identically distributed. Using our salespeople data as an example, we are assuming that the sales of one person in the data set is not influenced by the sales of another person in the data set. In this case, this seems like a reasonable assumption, and we will be making it for many (though not all) of the statistical methods used in this book. However, it is good to recognize that there are scenarios where this assumption cannot be made. For example, if the salespeople worked together in serving the same customers on the same products, and each individuals sales represented some proportion of the overall sales to the customer, we cannot say that the sales data is independent and identically distributed. In this case we will expect to see some hierarchy in our data and will need to adjust our techniques accordingly to take this into consideration. Under the central limit theorem, if we take samples from a random variable and calculate a summary statistic for each sample, that statistic is itself a random variable, and its mean converges to the true population statistic with more and more sampling. Let’s test this with a little experiment on our salespeople data. Figure 4.1 shows the results of taking 10, 100 and 1000 different random samples of 50, 100 and 150 salespeople from the salespeople dataset and creating a histogram of the resulting mean sales values. We can see how greater numbers of samples (down the rows) leads to a more normal distribution and larger sample sizes (across the columns) leads to a ‘spikier’ distribution with a smaller standard deviation. Figure 4.1: Histogram and density of mean sales based on sample sizes of 50, 100 and 150 (columns) and 10, 100 and 1000 samplings (rows) 4.2.2 Standard errors and confidence intervals One consequence of the observations in 4.1 is that the summary statistics calculated from larger sample sizes fall into normal distributions that are ‘narrower’ and hence represent more precise estimations of the population statistic. The standard deviation of of a sampled statistic is called the standard error of that statistic. In the special case of a sampled mean, the formula for the standard error of the mean can be derived to be \\[ SE = \\frac{\\sigma}{\\sqrt{n}} \\] Where is the (sample) standard deviation. This confirms that the standard error of the mean decreases with greater sample size, confirming our intuition that the estimation of the mean is more precise with larger samples. To apply this logic to our salespeople data set, let’s take a random sample of 100 values of customer_rate. N &lt;- length(salespeople$customer_rate[!is.na(salespeople$customer_rate)]) sample &lt;- salespeople$customer_rate[sample(1:N, 100)] We can calculate the mean of the sample and the standard error of the mean: # mean (sample_mean &lt;- mean(sample)) ## [1] 3.6828 # standard error (se &lt;- sd(sample)/sqrt(N)) ## [1] 0.04914091 Because the normal distribution is a frequency (or probability) distribution, we can interpret the standard error as a ‘sensitivity’ range around the sample mean corresponding to a probability level. One standard error above or below the sample mean corresponds to approximately 68% of the probability distribution, and two standard errors correspond to approximately 95% of the probability distribution. As an example, we can calculate a range of values from the sample mean and standard error that is 95% likely to contain the true population mean - the so called 95% confidence interval. ## 95% confidence interval lower and upper bounds lower_bound &lt;- sample_mean - 2*se upper_bound &lt;- sample_mean + 2*se cat(paste0(&#39;[&#39;, lower_bound, &#39;, &#39;, upper_bound, &#39;]&#39;)) ## [3.58451818592255, 3.78108181407745] "],
["hypothesis-testing.html", "4.3 Hypothesis testing", " 4.3 Hypothesis testing Observations about the distribution of statistics on random variables allow us to construct tests for hypotheses of statistical difference or similarity. Such hypothesis testing is useful in itself for simple bivariate analysis in practice settings, but it will be particularly critical in later chapters in determining whether models are useful or not. 4.3.1 Testing for a difference in sample means (Student’s t-test) Imagine that we are asked if the sales of low performers are the same as the sales of high performers in our salespeople data set. Let’s take two subsets of our data for those with a performance ratiing of 1 and those with a performance rating of 4, and calculate the difference in mean sales. # take two performance group samples perf1 &lt;- subset(salespeople, subset = performance == 1) perf4 &lt;- subset(salespeople, subset = performance == 4) # calculate the difference in mean sales (diff &lt;- mean(perf4$sales) - mean(perf1$sales)) ## [1] 154.9742 We can see that those with a higher performance rating in our sample did generate higher mean sales than those with a lower performance rating. But these are just samples, and we are being asked to give a conclusion about the populations they are drawn from. Let’s take a hypothesis that there is no difference in true mean sales between the two populations that these samples are drawn from. We call this the null hypothesis. We combine the two samples and calculate the distribution around the their difference in means. To accept the null hypothesis we would need to determine that the 95% confidence interval of this distribution contains zero. If it does not, we can reject the hypothesis in favour of the alternative hypothesis that there is a non-zero difference between the true mean sales of the two populations. We calculate the standard error of the combined sample using the fomula4: \\[ \\sqrt{\\frac{\\sigma_{\\mathrm{perf1}}^2}{n_{\\mathrm{perf1}}} + \\frac{\\sigma_{\\mathrm{perf4}}^2}{n_{\\mathrm{perf4}}}} \\] where \\(\\sigma_{\\mathrm{perf1}}\\) and \\(\\sigma_{\\mathrm{perf2}}\\) are the standard deviations of the two samples and \\(n_{\\mathrm{perf1}}\\) and \\(n_{\\mathrm{perf2}}\\) are the two sample sizes. This allows us to construct a 95% confidence interval for the difference between the means, and we can test whether this contains zero. # calculate standard error of the two sets se &lt;- sqrt(sd(perf1$sales)^2/length(perf1$sales) + sd(perf4$sales)^2/length(perf4$sales)) # calculate 95% confidence interval (approximately) (lower_bound &lt;- diff - 2*se) ## [1] 88.02317 (upper_bound &lt;- diff + 2*se) ## [1] 221.9253 # test if zero is inside this interval (0 &lt;= upper_bound) &amp; (0 &gt;= lower_bound) ## [1] FALSE Since this has returned FALSE, we conclude that a mean difference of zero is outside the 95% confidence interval of our sample mean difference, and so we cannot have 95% certainty that the difference in population means is zero. We reject the hypothesis that the mean sales of both performance levels are the same. Looking at this graphically, we are assuming a normal distribution of the mean difference, and we are determining where zero sits in that distribution, as in Figure 4.2. Figure 4.2: Distribution of the mean sales difference between perf1 and perf4 with 95% confidence intervals marked by red dashed lines and a mean difference of zero marked with a blue solid line The red dashed lines in this diagram represent the 95% confidence interval around the mean difference of our two samples. The ‘tails’ of the curve outside of these two lines each represent a maximum of 0.025 probability that zero is the population mean difference. So we can see that the position of the blue line can correspond to a maximum probability that the population mean difference is zero. We call this the p-value of the hypothesis test5. The p-value can be derived by calculating the position of zero as a proportion of the standard error of the sample mean difference (called the \\(t\\)-statistic) and then applying a conversion function, as follows6. # calculate t-statistic t &lt;- diff/se # convert to p-value (p &lt;- (1 - pnorm(abs(t), 0, 1))*2) ## [1] 3.665894e-06 This confirms an extremely low p-value. Nowadays, it is never necessary to do these calculations ourselves because hypothesis tests are a standard part of statistical software. In R, the t.test() function performs a hypothesis test of no mean difference on two samples7. t.test(perf4$sales, perf1$sales) ## ## Welch Two Sample t-test ## ## data: perf4$sales and perf1$sales ## t = 4.6295, df = 100.98, p-value = 1.093e-05 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 88.5676 221.3809 ## sample estimates: ## mean of x mean of y ## 619.8909 464.9167 Because of the small p-value calculated by the test, it rejects the null hypothesis and returns the alternative hypothesis. The accepted standard in the statistics community for rejection of a hypothesis is a p-value of less than 0.05. This standard is associated with the term statistically significant. Therefore we could say here that the two performance groups have a statistically significant difference in mean sales. In practice, there are numerous levels of p-value that are of interest to an analyst. While 0.05 is the most common standard in many disciplines, more stringent p-value standards of 0.01 and 0.001 are often used in situations where a high degree of certainty is desirable (for example some medical fields). Similarly, a less stringent p-value standard of 0.1 can be of interest particularly when sample sizes are small and the analyst is satisfied with ‘indications’ from the data. Many leading statisticians have argued - not without justification - that p-values are more a test of sample size than anything else and have cautioned against too much of a focus on p-values in making statistical conclusions from data. In particular, situations where data and methodology have been deliberately manipulated to achieve certain desired p-values - a process known as “p-hacking” - has been of increasing concern recently. 4.3.2 Testing for a non-zero correlation between two variables (t-test for correlation) Imagine that we are given a sample of data for two variables and we are asked if the variables are correlated in the overall population. We can take a null hypothesis that the variables are not correlated, determine a t-statistic associated with a zero correlation and convert this to a p-value. The t-statistic associated with correlation testing is often notated \\(t^*\\) and is defined as \\[ t^* = \\frac{r\\sqrt{n-2}}{1-r^2} \\] \\(t^*\\) can be converted to an associated p-value using a t-distribution in a similar way to how we compared population mean difference in the previous section8. The cor.test() function in R performs a hypothesis test on the null hypothesis that two variables have zero correlation. We can use it to test if sales and customer rating are correlated in our salespeople dataset. ## remove NAs from salespeople salespeople &lt;- salespeople[complete.cases(salespeople), ] cor.test(salespeople$sales, salespeople$customer_rate) ## ## Pearson&#39;s product-moment correlation ## ## data: salespeople$sales and salespeople$customer_rate ## t = 6.6952, df = 348, p-value = 8.648e-11 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.2415282 0.4274964 ## sample estimates: ## cor ## 0.337805 We see the null hypothesis has been rejected and we can conclude that there is a significant correlation between sales and customer rating. 4.3.3 Testing for a difference in frequency distribution between different categories in a data set (Chi-square test) Imagine that we are asked if the performance category of each person in the salespeople data set has a relationship with their promotion likelihood. We will test the null hypothesis that there is no difference in the distribution of promoted versus not promoted across the four performance categories. First we can produce a contingency table which is matrix containing counts of how many people were promoted or not promoted in each category. ## create contingency table of promoted versus performance (contingency &lt;- table(salespeople$promoted, salespeople$performance)) ## ## 1 2 3 4 ## 0 50 85 77 25 ## 1 10 25 48 30 We can see by summing each row that for the total sample we can expect 113 people to be promoted and 237 to miss out on promotion. We can use this ratio to compute an expected proportion in each performance category under the assumption that the distribution was exactly the same across all four categories. (expected_promoted &lt;- (sum(contingency[2, ])/sum(contingency)) * colSums(contingency)) ## 1 2 3 4 ## 19.37143 35.51429 40.35714 17.75714 (expected_notpromoted &lt;- (sum(contingency[1, ])/sum(contingency)) * colSums(contingency)) ## 1 2 3 4 ## 40.62857 74.48571 84.64286 37.24286 Now we can compare our observed versus expected values using the metric: \\[ \\frac{(\\mathrm{observed} - \\mathrm{expected})^2}{\\mathrm{expected}} \\] and add these all up to get a total, known as the \\(\\chi^2\\) statistic. notpromoted &lt;- sum((expected_notpromoted - contingency[1, ])^2/expected_notpromoted) promoted &lt;- sum((expected_promoted - contingency[2, ])^2/expected_promoted) (chi_sq_stat = notpromoted + promoted) ## [1] 25.89541 The \\(\\chi^2\\) statistic has an expected distribution that can be used to determine the p-value associated with this statistic. The distribution depends on the number of rows and columns in the contingency table9. In this case, a \\(\\chi^2\\) statistic of 25.895 is associated with a very low p-value, and so we can reject the null hypothesis and confirm the alternative hypothesis: that there is a difference in the distribution of promoted/not promoted individuals between the four performance categories. The chisq.test() function in R performs a chi-squared test of independence on a contingency table and returns the \\(\\chi^2\\) statistic and associated p-value for the null hypothesis. chisq.test(contingency) ## ## Pearson&#39;s Chi-squared test ## ## data: contingency ## X-squared = 25.895, df = 3, p-value = 1.003e-05 If you are inquisitive about this formula, see the exercises at the end of this chapter↩︎ We call this type of hypothesis test a two tailed test, because the tested population mean can be either higher or lower than the sample mean, thus it can appear in any of the two tails for the null hypothesis to be rejected. One-tailed tests are used when you are testing for an alternative hypothesis that the difference is specifically “less than zero” or “greater than zero”. In the t.test() function in R, you can specify this in the arguments↩︎ In R, this function uses pnorm() to calculate the value of the integral of the normal distribution between \\(-\\infty\\) and the absolute value of the \\(t\\)-statistic↩︎ Note some slight differences with the previous manual calculations, which did not have the same degree of precision↩︎ In this case the t-distribution has \\(n-2\\) degrees of freedom instead of the \\(n-1\\) degrees of freedom t-distribution used in comparing mean differences↩︎ The chi-square distribution depends on the degrees of freedom. This is calculated by subtracting one from the number of rows and from the number of columns and multiplying them together. In this case we have 2 rows and 4 columns which calculates to 3 degrees of freedom↩︎ "],
["learning-exercises-1.html", "4.4 Learning exercises", " 4.4 Learning exercises 4.4.1 Discussion questions Where relevant in these discussion exercises, let \\(X = x_1, x_2, ..., x_n\\) and \\(Y = y_1, y_2, ..., y_m\\) be samples of two random variables of length \\(n\\) and \\(m\\) respectively. If the values of \\(X\\) can only take the form 0 or 1, and if their mean is 0.25, how many of the values equal 0? If \\(m = n\\) and \\(X + Y\\) is formed from the element-wise sum of \\(X\\) and \\(Y\\), show that the mean of X + Y is equal to the sum of the mean of X and the mean of Y. For a scalar multiplier \\(a\\), show that \\(\\mathrm{Var}(aX) = a^2\\mathrm{Var}(X)\\). Explain why the standard deviation of X is a more intuitive measure of the deviation in X than the variance. Describe which two types of correlation you could use if X is an ordered ranking. Describe the role of sample size and sampling frequency in the distribution of sampling means for a random variable. Describe what a standard error is and how it relates to the probability density of a random variable statistic. If we conduct a t-test on the null hypothesis that \\(X\\) and \\(Y\\) are drawn from populations with the same mean, describe what a p-value of 0.01 means. Extension: The sum of variance law states that \\(\\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)\\). Use this together with the identity from Exercise 3 to derive the formula for the standard error of the mean of X: \\[ SE = \\frac{\\sigma_X}{\\sqrt{n}} \\] Extension: The sum of variance law also states that \\(\\mathrm{Var}(X - Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)\\). In a similar way to Exercise 9, show that the standard error for the difference between the means of \\(X\\) and \\(Y\\) is \\[ \\sqrt{\\frac{\\sigma_{X}^2}{n} + \\frac{\\sigma_{Y}^2}{m}} \\] 4.4.2 Data exercises For these exercises, download the charity donation dataset from https://raw.githubusercontent.com/keithmcnulty/eampa/master/data/charity_donation.csv. This dataset contains information on a sample of individuals who made donations to a nature charity. Calculate the mean total_donations from the data set. Calculate the sample variance for total_donation and convert this to a population variance. Calculate the sample standard deviation for total_donations and verify that it is the same as the square root of the sample variance. Calculate the sample correlation between total_donations and time_donating. By using an appropriate hypothesis test, determine if these two variables are independent in the overall population. Calculate the mean and the standard error of the mean for the first 20 entries of total_donations. Calculate the mean and the standard error of the mean for the first 50 entries of total_donations. Verify that the standard error is less than in Exercise 5. By using an appropriate hypothesis test, determine if the mean age of those who made a recent donation is different from those who did not. By using an appropriate hypothesis test, determine if there is a difference in whether or not a recent donation was made according to where people reside. Extension: By using an appropriate hypothesis test, determine if the age of those who have recently donated is at least 10 years older than those who have not recently donated in the population. Extension: By using an appropriate hypothesis test, determine if the average donation amount is at least ten dollars higher for those who recently donated versus those who did not. Retest for 20 dollars higher. "],
["linear-reg-ols.html", "Chapter 5 Elementary Linear Regression for Continuous Outcomes", " Chapter 5 Elementary Linear Regression for Continuous Outcomes In this chapter we will introduce and explore elementary linear regression, one of the first learning methods to be developed by statisticians and one of the easiest to interpret. Despite its simplicity - indeed because of its simplicity - it can be a very powerful tool in many situations. Linear regression will often be the first methodology to be trialled on a given problem, and will give an immediate benchmark with which to judge the efficacy of other, more complex, modeling techniques. Given the ease of interpretation, many analysts will select a linear regression model over more complex approaches even if those approaches produce a slightly better fit. This chapter will also introduce many critical concepts that will apply to other modeling approaches as we proceed through this book, therefore for inexperienced modelers this should be considered a foundational chapter which should not be skipped. "],
["when-ols.html", "5.1 When to use it", " 5.1 When to use it 5.1.1 Origins and Intuition of Linear Regression Linear Regression, also known as Ordinary Least Squares Linear Regression or OLS Regression for short, was developed independently by the mathematicians Gauss and Legendre at or around the first decade of the 19th century, and there remains today some controversy about who should take credit for its discovery. However, at the time of its discovery it was not actually known as “regression”. This term became more popular following the work of Francis Galton - a British intellectual jack-of-all-trades and a cousin of Charles Darwin. In the late 1800s, Galton had researched the relationship between the heights of a population of almost 1,000 children and the average height of their parents (mid-parent height). He was surprised to discover that the height of parents was not a perfect predictor of the height of children, and that in general children’s heights were more likely to be in a ‘narrower’ range that was closer to the mean for the total population. He described this statistical phenomenon as a “regression towards mediocrity” (“regression” comes from a Latin term approximately meaning “go back”). Here is an illustration of Galton’s data with the black solid line showing what a perfect relationship would look like, and the red dashed line showing the actual relationship he determined. You can regard the red dashed line as ‘going back’ from the perfect relationship (symbolized by the black line) in the direction of a flat line representing the mean of the child heights. This might give you an intuition that will help you understand later sections of this chapter. In an arbitrary data set, the red dashed line can lie anywhere between a flat line at the mean of \\(y\\) (no relationship) and the solid black line (a perfect relationship). Linear Regression is about finding the red dashed line in your data and using it to explain the degree to which your input data (the \\(x\\) axis) explains or predicts your outcome data (the \\(y\\) axis). Figure 5.1: Galton’s study of the height of children introduced the term ‘regression’ 5.1.2 Use cases for Linear Regression Linear regression is particularly suited to a problem where: The outcome of interest is on some sort of continuous scale (for example quantity, money, height, weight) There is reason to believe that the relationship between the outcome and the inputs can be approximated linearly In reality, number 2 is not that easy to determine with any certainty, although running some simple bivariate correlations between inputs and outcome can give a sense of linearity/non-linearity. Linear Regression can be a first port of call before trying more complex modeling approaches. It is simple and easy to explain, and analysts will often accept a somewhat poorer fit using Linear Regression in order to avoid having to interpret a more complex model. Here are some illustratory examples of questions that could be (at least initially) tackled with a Linear Regression approach: Given a data set of age, education level, education discipline, years of employment, industry employed in and current salary, to what extent can current salary be explained by the rest of the data? Given semi-annual test scores for a set of students over a three year period, what is the relationship between the final test score and earlier test scores? Given information on the demographics, personal characteristics and prior qualifications of a set of PhD students, to what degree can these explain the time taken to complete their studies? If your outcome of interest is on a binary or categorical scale (for example, ‘Yes’ or ‘No’; ‘High’, ‘Medium’ or ‘Low’), then linear regression is not recommended and logistic regression or other probabilistic methods are more well-suited, and we will cover these in subsequent chapters. 5.1.3 Walkthrough Example We will use walkthrough examples throughout most chapters of this book to maintain a focus on a real problem as we go through the essential theory and methodology, and to demonstrate the coding of the approach in R. The data sets for all walkthrough examples will be downloadable via the link provided. You are working as an analyst for the Biology Department of a large academic institution, which offers a four year Undergraduate degree program. The academic leaders of the department are interested in understanding how student performance in the final year examination of the degree program relates to performance in the prior three years. To help with this, you have been provided with data for 975 individuals graduating in the past three years, and you have been asked to create a model to explain each individual’s final examination score based on their examination scores for the first three years of their program. The Year 1 examination scores are awarded on a scale of 0-100, Years 2 and 3 on a scale of 0-200, and the Final year is awarded on a scale of 0-300. The dataset is here, and we will load it into our session and take a brief look at it. # obtain data from online csv at github url &lt;- &quot;https://raw.githubusercontent.com/keithmcnulty/eampa/master/data/ugtests.csv&quot; ugtests &lt;- read.csv(url) # look at the first few rows of data head(ugtests) ## Yr1 Yr2 Yr3 Final ## 1 27 50 52 93 ## 2 70 104 126 207 ## 3 27 36 148 175 ## 4 26 75 115 125 ## 5 46 77 75 114 ## 6 86 122 119 159 The data looks as expected, with test scores for four years all read in as numeric data types, but of course this is only a few rows. We need a quick statistical and structural overview of the data. # view structure str(ugtests) ## &#39;data.frame&#39;: 975 obs. of 4 variables: ## $ Yr1 : int 27 70 27 26 46 86 40 60 49 80 ... ## $ Yr2 : int 50 104 36 75 77 122 100 92 98 127 ... ## $ Yr3 : int 52 126 148 115 75 119 125 78 119 67 ... ## $ Final: int 93 207 175 125 114 159 153 84 147 80 ... # view statistical summary summary(ugtests) ## Yr1 Yr2 Yr3 Final ## Min. : 3.00 Min. : 6.0 Min. : 8.0 Min. : 8 ## 1st Qu.:42.00 1st Qu.: 73.0 1st Qu.: 81.0 1st Qu.:118 ## Median :53.00 Median : 94.0 Median :105.0 Median :147 ## Mean :52.15 Mean : 92.4 Mean :105.1 Mean :149 ## 3rd Qu.:62.00 3rd Qu.:112.0 3rd Qu.:130.0 3rd Qu.:175 ## Max. :99.00 Max. :188.0 Max. :198.0 Max. :295 We can see that the results do seem to have different scales in the different years as we have been informed, and judging by the means, students seem to have found Year 2 exams more challenging. We can also be assured that there is no missing data, as these would have been displayed as NA counts in our summary if they existed. We can also plot our four years of test scores pairwise to see any initial relationships of interest. library(ggplot2) library(GGally) # display a pair plot of all four columns of data GGally::ggpairs(ugtests) Figure 5.2: Pairplot of the ugtests data set In the diagonal we can see the distributions of the data in each column. We observe relatively normal looking distributions in each year. We can see scatter plots and pairwise correlation statistics off the diagonal. For example, we see a particularly strong correlation between Yr3 and Final test scores, a moderate correlation between Yr2 and Final and relative independence elsewhere. "],
["simple-ols.html", "5.2 Simple Linear Regression", " 5.2 Simple Linear Regression In order to visualize our approach and improve our intuition, we will start with simple linear regression, which is the case where there is only a single input variable and outcome variable. 5.2.1 Linear relationship between a single input and an outcome Let our input variable be \\(x\\) and our outcome variable be \\(y\\). Recalling the equation of a straight line, because we assume that the relationship is linear, we expect the relationship to be of the form: \\[y = mx + c\\] where \\(m\\) represents the slope or gradient of the line, and \\(c\\) represents the point at which the line intercepts the \\(y\\) axis. When using a straight line to model a relationship in the data, we call \\(c\\) and \\(m\\) the coefficients of the model. Now let’s assume that we have a sample of ten datapoints with which to estimate our linear relationship. Let’s take the first ten values of Yr3 and Final in our ugtests dataset: (d &lt;- head(ugtests[ , c(&quot;Yr3&quot;, &quot;Final&quot;)], 10)) ## Yr3 Final ## 1 52 93 ## 2 126 207 ## 3 148 175 ## 4 115 125 ## 5 75 114 ## 6 119 159 ## 7 125 153 ## 8 78 84 ## 9 119 147 ## 10 67 80 We can do a simple plot of these data points as in Figure 5.3: Figure 5.3: Basic scatter plot of ten datapoints Intuitively, we can imagine a line passing through these points that ‘fits’ the general pattern. For example, taking \\(m = 1.2\\) and \\(c = 5\\), the resulting line \\(y = 1.2x + 5\\) could fit between the points we are given, as displayed in Figure 5.4: Figure 5.4: Fitting \\(y=1.2x + 5\\) to our ten datapoints This looks like an approximation of the relationship, but how do we know that it is the best approximation? 5.2.2 Minimising the error For each of our data points, we can determine an error in the fitted model by calculating the difference between the real value of \\(y\\) and the one predicted by our model. For example, at \\(x = 52\\), our modeled value of y is , but the real value is 93, producing an error of . These errors are known as the residuals of our model. The residuals for the ten points in our dataset are illustrated by the solid red line segments in Figure 5.5. It looks like at least one of our residuals is pretty large. Figure 5.5: Residuals of \\(y=1.2x + 5\\) for our ten datapoints The total error of our model, which we want to minimize could be defined in a number of ways: The sum of the residuals The sum of the absolute values of our residuals (so that negative values are converted to positive values) The sum of the squares of our residuals (note that all squares are positive) For a number of reasons (not least that fact that at the time this method was developed it was one of the easiest to calculate manually), the most common approach is number 3, which is why we call our regression model Ordinary Least Squares regression. Some algebra and calculus can help us determine the equation of the line that generates the least squared residual error. You can see more of the mathematics here, but let’s look at how this works in practice. 5.2.3 Determining the best fit We can run a fairly simple function in R to calculate the best fit linear model for our data. Once we have run that function, the model, and all the details, will be saved in our session for further investigation or use. First we need to express the model we are looking to calculate as a formula. In this simple case we want to regress the outcome \\(y =\\) Final against the input \\(x =\\) Yr3, and therefore we would use the simple formula notation Final ~ Yr3. Now we can use the lm() function to calculate the OLS model based on our dataset and our formula. ## calculate model model &lt;- lm(formula = Final ~ Yr3, data = d) The model object that we have created is a list of a number of different pieces of information, which we can see by looking at the names of the objects in the list: names(model) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; ## [8] &quot;df.residual&quot; &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; So we can already see some terms we are familiar with. For example, we can look at the coefficients: model$coefficients ## (Intercept) Yr3 ## 16.630452 1.143257 This tells us that that our best fit model - the one that minimises the sum of the squares of the residuals - is \\(y = 1.14x + 16.63\\). In other words, our Final test score can be expected to take a value of 16.63 even with zero score in the Yr3 input, and that every additional point scored in Yr3 will increase the Final score by 1.14. 5.2.4 Measuring the fit of the model We have calculated a model which minimizes the sum of squares residual error for the sample of data that we have, but we don’t really have a sense of how ‘good’ the model is. How do we tell how well our model uses the input data to explain the outcome? This is an important question to answer because you would not want to propose a model that does not do a good job of explaining your outcome, and you also may need to compare your model to other alternatives, which will require some sort of benchmark metric. One natural way to benchmark how good a job your model does of explaining the outcome is to compare it to a situation where you have no input and no model at all. In this situation, all you have is your outcome values which can be considered a random variable with a mean and a variance. In the case of our ten datapoints, we have ten values of Final with a mean of 133.7. We can consider the horizontal line representing the mean of \\(y\\) as our ‘random model’, and we can calculate the residuals around the mean. This can be seen in Figure 5.6. Figure 5.6: Residuals of our ten datapoints around their mean value Recall from our previous chapter the definition of the population variance of \\(y\\), notated as \\(\\mathrm{Var}(y)\\). Note that it is defined as the average of the squares of the residuals around the mean of\\(y\\). Therefore \\(\\mathrm{Var}(y)\\) represents the mean sum of squares error of a random model. This calculates in this case to 1457.6. Now let’s overlay our fitted model onto this random model in Figure 5.7: Figure 5.7: Comparison of residuals of fitted model (red) against random variable (blue) So for most of our observations (though not all) we seem to have reduced the ‘distance’ from the random model by fitting our new model. If we average the square of our residuals for the fitted model, we obtain the mean sum of squares error of our fitted model, which calculates to 663.91. Therefore before we fit our model, we have an average error of 1457.6, and after we fit it, we have an average error of 663.91. So we have reduced the average error of our model by 793.69 or, expressed as a proportion, by 0.54. In other words, we can say that our model explains 0.54 (or 54%) of the variance of our outcome. This metric is known as the \\(R^2\\) of our model and is the primary metric used in measuring the fit of a linear regression model10. As a side note, in a simple regression model like this, where there is only one input variable, we have the simple identity \\(R^2 = r^2\\), where \\(r\\) is the correlation between the input and outcome (for our small set of ten datapoints here, the correlation is 0.864)↩︎ "],
["multiple-linear-regression.html", "5.3 Multiple linear regression", " 5.3 Multiple linear regression In reality, regression problems rarely involve one single input variable, but rather multiple variables. The methodology for multiple linear regression is similar in nature to a simple linear regression, but obviously more difficult to visualize because of its increased dimensionality. In this case, our input is a vector of \\(p\\) variables \\((x_1, x_2, ..., x_p)\\). Extending the linear equation in Figure 5.2.1, we seek to develop an equation of the form: \\[y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p\\] so that our sum of squares residual error is minimized. 5.3.1 Running a multiple linear regression model and interpreting its coefficients A multiple linear regression model is run is a similar way to a simple regression model, with your formula notation determining what outcome and input variables you wish to have in your model. Let’s now perform a multiple linear regression on our entire ugtests dataset and regress our Final test score against all prior test scores using the formula Final ~ Yr3 + Yr2 + Yr1 and determine our coefficients as before. model &lt;- lm(data = ugtests, formula = Final ~ Yr3 + Yr2 + Yr1) model$coefficients ## (Intercept) Yr3 Yr2 Yr1 ## 14.14598945 0.86568123 0.43128539 0.07602621 Referring to our formula in Section 5.3, let’s understand what each coefficient \\(\\beta_0, \\beta_1, ..., \\beta_p\\) means. \\(\\beta_0\\), the intercept of the model, represents the value of \\(y\\) assuming that all the inputs were zero. You can imagine that your output can be expected to have a base value even without any inputs - a student who completely flunked the first three years can still redeem themselves to some extent in the Final year. Now looking at the other coefficients, let’s consider what happens if our first input \\(x_1\\) increased by a single unit, assuming nothing else changed. We would then expect our value of y to increase by \\(\\beta_1\\). Similarly for any input \\(x_k\\), a unit increase would result in an increase in \\(y\\) of \\(\\beta_k\\), assuming no other changes in the inputs. In the case of our ugtests dataset, we can say the following: The intercept of the model is 14.146. This is the value that a student could be expected to score in their final exam even if they had scored zero in all previous exams. The Yr3 coefficient is 0.866. Assuming no change in other inputs, this is the increase in the Final exam score that could be expected from an extra point in the Year 3 score. The Yr2 coefficient is 0.431. Assuming no change in other inputs, this is the increase in the Final exam score that could be expected from an extra point in the Year 2 score. The Yr1 coefficient is 0.076. Assuming no change in other inputs, this is the increase in the Final exam score that could be expected from an extra point in the Year 1 score. 5.3.2 Coefficient confidence Intuitively, these coefficients appear too precise for comfort. After all, we are attempting to estimate a relationship based on a limited set of data. In particular, looking at the Yr1 coefficient, it seems to be very close to zero, implying that there is a possibility that the Year 1 examination score has no impact on the final examination score. Like in any statistical estimation, the coefficients calculated for our model have a margin of error. Typically, in any such situation, we seek to know a 95% confidence interval to set a standard of certainty around the values we are interpreting. The summary() function is a useful way to gather critical information in your model, including important statistics on your coefficients: model_summary &lt;- summary(model) model_summary$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.14598945 5.48005618 2.581358 9.986880e-03 ## Yr3 0.86568123 0.02913754 29.710169 1.703293e-138 ## Yr2 0.43128539 0.03250783 13.267124 4.860109e-37 ## Yr1 0.07602621 0.06538163 1.162807 2.451936e-01 The 95% confidence interval corresponds to approximately two Standard Errors above or below the estimated value. For a given coefficient, if this confidence interval includes zero, you cannot reject the hypothesis that the variable has no relationship with the outcome. Another indicator of this is the Pr(&gt;|t|) column of the coefficient summary, which represents the p-value of the hypothesis test that the input variable has no relationship with the outcome. If this value is less than a certain threshold (usually 0.05), you can conclude that this variable has a statistically significant relationship with the outcome. To see the precise confidence intervals for your model coefficients, you can use the confint() function: confint(model) ## 2.5 % 97.5 % ## (Intercept) 3.39187185 24.9001071 ## Yr3 0.80850142 0.9228610 ## Yr2 0.36749170 0.4950791 ## Yr1 -0.05227936 0.2043318 In this case, we can conclude that the examinations in Years 2 and 3 have a significant relationship with the Final examination score, but we cannot conclude this for Year 1. Effectively, this means that we can drop Yr1 from our model with no substantial loss of fit. In general, simpler models are easier to manage and interpret, so let’s remove the non-significant variable now. newmodel &lt;- lm(data = ugtests, formula = Final ~ Yr3 + Yr2) Given that our new model only has three dimensions, we have the luxury of visualizing it. Interactive Figure 5.8 shows the data and the fitted plane of our model. Figure 5.8: 3D Visualization of the fitted newmodel against the ugtests data 5.3.3 Model confidence At this point we can further explore the overall summary of our model. As you saw in the previous section, our model summary contains numerous objects of interest, including statistics on the coefficients of our model. We can see what is inside our summary by looking at the names of its contents, and we can then dive in and explore specific objects of interest. For example: ## get summary of model newmodel_summary &lt;- summary(newmodel) ## see summary contents names(newmodel_summary) ## [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; ## [8] &quot;r.squared&quot; &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; ## view r-squared newmodel_summary$r.squared ## [1] 0.5296734 We can see that our model explains a lot of the variance in the Final examination score. Alternatively, we can view the entire summary to receive a formatted report on our model. ## see full model summary newmodel_summary ## ## Call: ## lm(formula = Final ~ Yr3 + Yr2, data = ugtests) ## ## Residuals: ## Min 1Q Median 3Q Max ## -91.12 -20.36 -0.22 18.94 98.29 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 18.08709 4.30701 4.199 2.92e-05 *** ## Yr3 0.86496 0.02914 29.687 &lt; 2e-16 *** ## Yr2 0.43236 0.03250 13.303 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 30.44 on 972 degrees of freedom ## Multiple R-squared: 0.5297, Adjusted R-squared: 0.5287 ## F-statistic: 547.3 on 2 and 972 DF, p-value: &lt; 2.2e-16 This provides us with some of the most important metrics from our model. In particular, the last line gives us a report on our model confidence - that is, how confident can we be that our model fits the outcome better than the alternative of a random model. The F-statistic is an evaluation of the confidence of the entire model, with a higher F-statistic indicating a strong likelihood that the model fits the data better than a random model. More intuitively, perhaps, we also have the p-value for the F-statistic. In this case it is extremely small, so we can conclude that our model has some explanatory/predictive power over and above a random model. Be careful not to confuse model confidence with \\(R^2\\). Depending on your sample, it is entirely possible for a model with a low \\(R^2\\) to have high confidence and vice versa. 5.3.4 Making predictions from your model While the book focuses on inferential rather than predictive analytics, we briefly touch here on the mechanics of generating predictions from models. As you might imagine, once the model has been fitted, prediction is a relatively straightforward process. We feed in the Yr2 and Yr3 examination scores into our fitted model, and it applies the coefficients to calculate the predicted outcome. Let’s look at three fictitious students, and create a dataframe with their scores to input into the model. (new_students &lt;- data.frame( Yr2 = c(67, 23, 88), Yr3 = c(144, 100, 166) )) ## Yr2 Yr3 ## 1 67 144 ## 2 23 100 ## 3 88 166 Now we can feed these values into our model to get predictions of the Final examination result for our three new students. ## use newmodel to predict for new_students predict(newmodel, new_students) ## 1 2 3 ## 171.6093 114.5273 199.7179 We know from our earlier work in this chapter that there is a confidence interval around the coefficients of our model, which means that there is a range of values for our prediction according to those confidence intervals. This can be determined by specifying that you require a confidence interval: ## get a confidence interval predict(newmodel, new_students, interval = &quot;confidence&quot;) ## fit lwr upr ## 1 171.6093 168.2125 175.0061 ## 2 114.5273 109.7081 119.3464 ## 3 199.7179 195.7255 203.7104 You may also recall from our previous chapter on modeling theory that any datapoint in our outcome is subject to uncontrollable error, so that there is a further margin of ‘prediction error’, even after we take into consideration the confidence interval of our model. Therefore to generate a more reliable prediction range to use in real life, which takes this random, uncontrollable error into consideration, you should calculate a ‘prediction interval’: ## get a prediction interval predict(newmodel, new_students, interval = &quot;prediction&quot;) ## fit lwr upr ## 1 171.6093 111.77795 231.4406 ## 2 114.5273 54.59835 174.4562 ## 3 199.7179 139.84982 259.5860 You will notice that the prediction interval is substantially wider than the confidence interval. However, in human processes where there is often room for judgment, the prediction interval can be a valuable piece of information. In our example here, the Faculty may wish to provide their staff with some discretion on the Final examination score based on their observations of the individual in class, or on their perception of their learning ‘trajectory’. Providing a recommended score and a range around that score can help facilitate, but also control, this discretionary element of the process. In reality, the process of developing a model to predict an outcome can be quite different from developing a model to explain an outcome. For a start, it is unlikely that you would use your entire sample to fit a predictive model, as you would want to reserve a portion of your data to test for its fit on new data. Since the focus of this book is inferential modeling, much of this topic will be out of our scope. "],
["managing-inputs-in-linear-regression.html", "5.4 Managing inputs in linear regression", " 5.4 Managing inputs in linear regression Our walkthrough example for this chapter, while useful for illustrating the key concepts, is a very straightfoward data set to run a model on. There is no missing data, and all the data inputs have the same numeric data type (in the exercises at the end of this chapter we will present a more varied data set for analysis). Commonly, an analyst will have a list of possible input variables that they can consider in their model, and rarely will they run a model using all of these variables. In this section we will cover some common elements of decision making and design of input variables in regression models. 5.4.1 Relevance of input variables The first step in managing your input variables is to make a judgment about their relevance to the outcome being modeled. Analysts should not blindly run a model on a set of variables before considering their relevance. There are two common reasons for rejecting the inclusion of an input variable: There is no reasonable possibility of a direct or indirect causal relationship between the input and the outcome. For example, if you were provided with the height of each individual taking the Final examination in our walkthrough example, it would be difficult to see how that could reasonably relate to the outcome that you are modeling. If there is a possibility that the model will be used to predict based on new data in the future, there may be variables that you explicity do not wish to be used in any prediction. For example, if our walkthrough model contained student gender data, we would not want to include that in a model that predicted future student scores because we would not want gender to be taken into consideration when determining student performance. 5.4.2 Sparseness (‘missingness’) of data Missing data is a very common problem in modeling. If an observation has missing data in a variable that is being included in the model, that observation will be ignored, or an error will be thrown. This forces a model trained on a smaller set of data, which can compromise fit and predictive accuracy. Running summary functions on your data (eg summary() in R) will reveal variables that contain missing data if it exists. There are three main options for how missing data is handled: If the data for a given variable is relatively complete and only a small number of observations are missing, its usually best and simplest to remove the observations that are missing from the dataset. Note that many modeling functions (though not all) will take care of this automatically. As data becomes more sparse, removing observations becomes less of an option. If the sparseness is massive (eg more than half of the data is missing), then there is no choice but to remove that variable from the model. While this may be unsatisfactory for a given variable (because it is thought to have an important explanatory role), the fact remains that data that is mostly missing is not a good measure of that variable in the first place. Moderate sparse data could be considered for imputation. Imputation methods involve using the overall statistical properties of the entire data set or of specific other variables to ‘suggest’ what the missing value might be, ranging from simple mean and median values to more complex imputation methods. Imputation methods are more commonly used in predictive settings, and we will not cover imputation methods in depth here. 5.4.3 Transforming categorical inputs to dummy variables Many models will have categorical inputs rather than numerical inputs. Categorical inputs usually take forms such as: Binary values - for example, Yes/No, True/False Unordered categories - for example Car, Train, Bicycle Ordered categories - for example Low, Medium, High Categorical variables do not behave like numerical variables. There is no sense of quantity in a categorical variable. We do not know how a Car relates to a Train quantitatively, we only know that they are different. Even for an ordered category, although we know that ‘Medium’ is higher than ‘Low’, we do not know how much higher or indeed whether the difference is the same as that between ‘High’ and ‘Medium’. In general, all model input variables should take a numeric form. The most reliable way to do this is to convert categorical values to dummy variables. While some packages and function have a built-in ability to convert categorical data to dummy variables, not all do, so it is important to know how to do this yourself. Consider the following data set: (vehicle_data &lt;- data.frame( make = c(&quot;Ford&quot;, &quot;Toyota&quot;, &quot;Audi&quot;), manufacturing_cost = c(15000, 19000, 28000) )) ## make manufacturing_cost ## 1 Ford 15000 ## 2 Toyota 19000 ## 3 Audi 28000 The make data is categorical, so it will be converted to several columns for each possible value of make and binary labelling will be used to identify whether that value is present in that specific observation. Many packages and functions are available to conveniently do this, for example: library(dummies) (dummy_vehicle &lt;- dummies::dummy(&quot;make&quot;, data = vehicle_data)) ## makeAudi makeFord makeToyota ## 1 0 1 0 ## 2 0 0 1 ## 3 1 0 0 Dummy variables can then replace the original make column to get your data set ready for modeling: (vehicle_data_dummies &lt;- cbind( manufacturing_cost = vehicle_data$manufacturing_cost, dummy_vehicle )) ## manufacturing_cost makeAudi makeFord makeToyota ## 1 15000 0 1 0 ## 2 19000 0 0 1 ## 3 28000 1 0 0 It is worth a moment to consider how to interpret coefficients of dummy variables in a linear regression model. If \\(x_k\\) is one of our dummy variables, then it can only take a value of 0 or 1. Therefore its coefficient \\(\\beta_k\\) is either ‘switched off’ or ‘switched on’ in the model. If we were to try to use the data in our vehicle_data_dummies dataset to predict, say, the retail price of a vehicle, we would interpret coefficients like this: For each extra dollar spent on manufacturing, the retail price increases by … If the vehicle is a Ford, the retail price increases/decreases by … If the vehicle is a Toyota, the retail price increases/decreases by … If the vehicle is an Audi, the retail price increases/decreases by … This highlights the importance of appropriate interpretation of coefficients, and in particular the proper understanding of units. It will be common to see much larger coefficients for dummy variables in regression models, because they represent a binary ‘all’ or ‘nothing’ variable in the model. The coefficient for, say, manufacturing cost, would be much smaller because a unit in this case is a dollar of manufacturing spend, on a scale of many thousands of potential dollars in spend. Care should be taken not to ‘rank’ coefficients by their value. Higher coefficients in and of themselves do not imply greater importance. "],
["testing-your-model-assumptions.html", "5.5 Testing your model assumptions", " 5.5 Testing your model assumptions All modeling techniques have underlying assumptions about the data that they model, and can generate inaccurate results when those assumptions do not hold true. Conscientious analysts will verify that these assumptions are satisfied before finalizing their modeling efforts. In this section we will outline some common diagnostics of model assumptions when running linear regression models. 5.5.1 Assumption of linearity and additivity We mentioned earlier that linear regression assumes that the relationship we are trying to model is linear and additive in nature. Therefore you can expect problems if you are using this approach to model a pattern that is non-linear. In general, you’d expect this problem to come out ‘in the wash’ anyway, because a linear model will not produce a good fit on a non-linear underlying relationship. Nevertheless, you can check whether your linearity assumption was reasonable in a couple of ways. You can plot the true versus the predicted (fitted) values to see if they look correlated. You can see such a plot on our student examination model from before in Figure 5.9: predicted_values &lt;- newmodel$fitted.values true_values &lt;- ugtests$Final #plot true values against predicted values plot(predicted_values, true_values) Figure 5.9: Plot of true versus fitted/predicted student scores Alternatively, you can plot the residuals of your model against the predicted values, and look for the pattern of a random distribution (ie no major discernible pattern), such as in Figure 5.10: residuals &lt;- newmodel$residuals #plot residuals against predicted values plot(predicted_values, residuals) Figure 5.10: Plot of residuals against fitted/predicted scores You can also plot the residuals against each input variable as an extra check of independent randomness, looking for a reasonably random distribution in all cases. If you find that your residuals are following a clear pattern and are not random in nature, this is an indication that a linear model is not a good choice for your data. 5.5.2 Assumption of constant error variance It is assumed in a linear model that the errors or residuals are homoscedastic - this means that their variance is constant across the values of the input variables. If the errors of your model heteroscedastic - that is, if they increase or decrease according to the value of the model inputs, this can lead to poor estimations of confidence intervals and fits. While a simple plot of residuals against predicted values (such as in Figure 5.10) can give a quick indication on homoscedacity, to be thorough the residuals should be plotted against each input variable, and it should be verified that the range of the residuals remains broadly stable. In our student examination model, we can first plot the residuals against the values of Yr2 in Figure 5.11: Yr2 &lt;- ugtests$Yr2 #plot residuals against Yr2 values plot(Yr2, residuals) Figure 5.11: Plot of residuals against Yr2 values Then we can plot against Yr3 in Figure 5.12: Yr3 &lt;- ugtests$Yr3 #plot residuals against Yr3 values plot(Yr3, residuals) Figure 5.12: Plot of residuals against Yr3 values Both plots show a pretty consistent range of values which reassures us that we have homoscedacity. 5.5.3 Assumption of normally distributed errors In an appropriate model we expect our errors to be random, so we would therefore expect our residuals to be normally distributed over sufficient numbers of observations. If our residuals are distributed differently, this is again an indicator of an inappropriate model and can result in inaccurate estimates of confidence intervals and the statistical significance of coefficients. The quickest way to determine if residuals in your sample are consistent with a normal distribution is to run a quantile-quantile plot (or QQplot) on the residuals. This will plot the observed quantiles of your sample against the theoretical quantiles of a normal distribution. The closer this plot looks like a perfect correlation, the more certain you can be that this normality assumption holds. An example for our student examination model is in Figure 5.13: ## normal distribution qqplot of residuals qqnorm(newmodel$residuals) Figure 5.13: Quantile-Quantile plot of residuals 5.5.4 Avoiding high collinearity and multicollinearity between input variables In multiple linear regression, the various input variables used can be considered ‘dimensions’ of the problem or model. In theory, we ideally expect dimensions to be independent and uncorrelated. Practically speaking, however, it’s very challenging in large data sets to ensure that every input variable is completely uncorrelated from another. For example, even in our limited ugtests data set we saw in Figure 5.2 that Yr2 and Yr3 examination scores are correlated to some degree. While some intercorrelation between input variables can be expected and tolerated in linear regression models, high levels of correlation can result in significant inflation of coefficients and inaccurate estimates of p-values of coefficients. Collinearity means that two input variables are highly correlated. The definition of ‘high correlation’ is a matter of judgment, but as a rule of thumb correlations greater than 0.5 might be considered high and greater than 0.7 might be considered extreme. Creating a simple correlation matrix or a pairplot (such as Figure 5.2) can immediately surface high or extreme collinearity. Multicollinearity means that there is a linear relationship between more than two of the input variables. This may not always present itself in the form of high correlations between pairs of input variables, but may be seen by identiying ‘clusters’ of moderately correlated variables, or by calculating a Variance Inflation Factor (VIF) for each input variable - where VIFs greater than 5 indicate high multicollinearity. Easy-to-use tests also exist in statistical software for identifying multicollinearity (for example the mctest package in R). Here is how we would test for multicollinearity in our student examination model. library(mctest) #diagnose possible overall presence of multicollinearity mctest::omcdiag(newmodel) ## ## Call: ## mctest::omcdiag(mod = newmodel) ## ## ## Overall Multicollinearity Diagnostics ## ## MC Results detection ## Determinant |X&#39;X|: 0.9981 0 ## Farrar Chi-Square: 1.8365 0 ## Red Indicator: 0.0434 0 ## Sum of Lambda Inverse: 2.0038 0 ## Theil&#39;s Method: -0.5259 0 ## Condition Number: 9.1952 0 ## ## 1 --&gt; COLLINEARITY is detected by the test ## 0 --&gt; COLLINEARITY is not detected by the test #if necessary, diagnose specific multicollinear variables using VIF mctest::imcdiag(newmodel, method = &quot;VIF&quot;) ## ## Call: ## mctest::imcdiag(mod = newmodel, method = &quot;VIF&quot;) ## ## ## VIF Multicollinearity Diagnostics ## ## VIF detection ## Yr3 1.0019 0 ## Yr2 1.0019 0 ## ## NOTE: VIF Method Failed to detect multicollinearity ## ## ## 0 --&gt; COLLINEARITY is not detected by the test ## ## =================================== Note that collinearity and multicollinearity only affect the coefficients of the variables impacted, and do not affect other variables or the overall statistics and fit of a model. Therefore, if a model is being developed primarily to make predictions and there is little interest in using the model to explain a phenomenon, there may not be any need to address this issue at all. However, in inferential modeling the accuracy of the coefficients is very important, and so testing of multicollinearity is essential. In general, the best way to deal with collinear variables is to remove one of them from the model (usually the one that has the least significance in explaining the outcome). "],
["extending-multiple-linear-regression.html", "5.6 Extending multiple linear regression", " 5.6 Extending multiple linear regression We wrap up this chapter by introducing some simple extensions of linear regression, with a particular aim of trying to improve the overall fit of a model by relaxing the linear or additive assumptions. It is rare for practitioners to extend linear regression models too greatly due to the negative impact this can have on interpretation, but simple extensions such as experimenting with interaction terms or quadratics are not uncommon. If you have an appetite to explore this topic more fully, I recommend C. Radhakrishna Rao (2008). 5.6.1 Interactions between input variables Recall that our model of student examination scores took each year’s score as an independent input variable, and therefore we are making the assumption that the score obtained in each year acts independently and additively in predicting the Final score. However, it is very possible that several input variables act together in relation to the outcome. One way of modelling this is to include interaction terms in your model, which are new input variables formed as products of the original input variables. In our students examination data in ugtests, we could consider extending our model to not only include the individual years examinations, but also to include the impact of combined changes across multiple years. For example, we could combine the impact of Yr2 and Yr3 examinations by multiplying them together in our model. interaction_model &lt;- lm(data = ugtests, formula = Final ~ Yr2 + Yr3 + Yr2*Yr3) summary(interaction_model) ## ## Call: ## lm(formula = Final ~ Yr2 + Yr3 + Yr2 * Yr3, data = ugtests) ## ## Residuals: ## Min 1Q Median 3Q Max ## -78.084 -18.284 -0.546 18.395 79.824 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.320e+02 1.021e+01 12.928 &lt; 2e-16 *** ## Yr2 -7.947e-01 1.056e-01 -7.528 1.18e-13 *** ## Yr3 -2.267e-01 9.397e-02 -2.412 0.0161 * ## Yr2:Yr3 1.171e-02 9.651e-04 12.134 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.38 on 971 degrees of freedom ## Multiple R-squared: 0.5916, Adjusted R-squared: 0.5903 ## F-statistic: 468.9 on 3 and 971 DF, p-value: &lt; 2.2e-16 We see that introducing this interaction term has improved the fit of our model from 0.53 to 0.59, and that the interaction term is significant, so we conclude that in addition to a significant effect of the Yr2 and Yr3 scores, there is an additional significant effect from their interaction Yr2*Yr3. Let’s take a moment to understand how to interpret this, since we note that some of the coefficients are now negative. Our model now includes two input variables and their interaction, so it can be written as \\[ \\begin{align*} \\mathrm{Final} &amp;= \\beta_0 + \\beta_1\\mathrm{Yr3} + \\beta_2\\mathrm{Yr2} + \\beta_3\\mathrm{Yr3}\\mathrm{Yr2} \\\\ &amp;= \\beta_0 + (\\beta_1 + \\beta_3\\mathrm{Yr2})\\mathrm{Yr3} + \\beta_2\\mathrm{Yr2} \\\\ &amp;= \\beta_0 + \\gamma\\mathrm{Yr3} + \\beta_2\\mathrm{Yr2} \\end{align*} \\] where \\(\\gamma = \\beta_1 + \\beta_3\\mathrm{Yr2}\\). Therefore our model has coefficients which are not constant, but change with the values of the input variables. We can conclude that the effect of an extra point in the examination in Year 3 will be different depending on how the student performed in Year 2. Visualizing this, we can see in Interactive Figure 5.14 that this non-constant term introduces a curvature to our fitted surface that aligns it a little more closely with the observations in our data set. Figure 5.14: 3D Visualization of the fitted interaction_model against the ugtests data By examining the shape of this curved plane, we can observe that the model considers trajectories in the Year 2 and Year 3 examination scores. Those individuals who have improved from one year to the next will perform better in this model than those who declined. To demonstrate, lets look at the predicted scores from our interaction model for someone who declined and for someone who improved from Year 2 to Year 3. # data frame with a declining and an improving observation obs &lt;- data.frame( Yr2 = c(150, 75), Yr3 = c(75, 150) ) predict(interaction_model, obs) ## 1 2 ## 127.5010 170.1047 Through including the interaction effect, the model interprets declining examination scores more negatively than improving examination scores. This kinds of additional inferential insights may be of great interest. However, consider the impact on interpretability of modeling too many combinations of interactions. As always, there is a trade-off between intepretability and accuracy11. When running models with interaction terms, you can expect to see a hierarchy in the coefficients according to the level of the interaction. For example, single terms will usually generate higher coefficients than interactions of two terms, which will generate higher coefficients than interactions of three terms, and so on. Given this, whenever an interaction of terms is considered significant in a model, then the single terms contained in that interaction should automatically be regarded as significant. 5.6.2 Quadratic and higher-order polynomial terms In many situations the real underlying relationship between the outcome and the inputs may be non-linear. For example, if the underlying relationship was throught to be quadratic on a given input variable \\(x\\), then the formula would take the form \\(y = \\beta_0 + \\beta_1x + \\beta_2x^2\\). We can easily trial polynomial terms using our linear model technology. For example, recall that we removed Yr1 data from our model because it was not significant when modeled linearly. We could test if a quadratic model on Yr1 helps improve our fit12: quadratic_yr1_model &lt;- lm(data = ugtests, formula = Final ~ Yr3 + Yr2 + Yr1 + I(Yr1^2)) summary(quadratic_yr1_model) ## ## Call: ## lm(formula = Final ~ Yr3 + Yr2 + Yr1 + I(Yr1^2), data = ugtests) ## ## Residuals: ## Min 1Q Median 3Q Max ## -92.409 -20.363 -0.021 18.934 98.190 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.896247 9.243879 1.179 0.239 ## Yr3 0.865515 0.029152 29.690 &lt;2e-16 *** ## Yr2 0.431605 0.032530 13.268 &lt;2e-16 *** ## Yr1 0.213160 0.320806 0.664 0.507 ## I(Yr1^2) -0.001330 0.003047 -0.437 0.662 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 30.45 on 970 degrees of freedom ## Multiple R-squared: 0.5304, Adjusted R-squared: 0.5285 ## F-statistic: 273.9 on 4 and 970 DF, p-value: &lt; 2.2e-16 In this case we find that modeling Yr1 as a quadratic makes no difference to the fit of the model. References "],
["learning-exercises-2.html", "5.7 Learning exercises", " 5.7 Learning exercises 5.7.1 Discussion questions What is the approximate meaning of the term ‘regression’? Why is the term particularly suited to the methodology described in this chapter? What basic condition must the outcome variable satisfy for linear regression to be a potential modeling approach? Describe some ideas for problems that might be modeled using linear regression. What is the difference between simple linear regression and multiple linear regression? What is a residual and how does it relate to the term ‘Ordinary Least Squares’? How are the coefficients of a linear regression model interpreted? Explain why higher coefficients do not necessarily imply greater importance. How is the \\(R^2\\) of a linear regression model interpreted? What are the minimum and maximum possible values for \\(R^2\\) and what does each mean? What are the key considerations when preparing input data for a linear regression model? Describe what you understand by the term ‘dummy variable’. Why are dummy variable coefficients often larger than other coefficients in linear regression models? Describe the term ‘collinearity’ and why it is an important consideration in regression models. Describe some ways that linear regression models can be extended into non-linear models. 5.7.2 Data exercise The sociological_data dataset can be obtained at https://raw.githubusercontent.com/keithmcnulty/eampa/master/data/sociological_data.csv. This data represents a sample of information obtained from individuals who participated in a global research study, and contains the following fields: annual_income_ppp: The annual income of the individual in PPP adjusted US dollars average_wk_hrs: The average number of hours per week worked by the individual education_months: The total number of months spend by the individual in formal primary, secondary and tertiary education region: The region of the world where the individual lives job_type: Whether the individual works in a skilled or unskilled profession gender: The gender of the individual family_size: The size of the individual’s family of dependents work_distance: The distance between the indivdual’s residence and workplace in kilometers languages: The number of languages spoken fluently by the individual Conduct some exploratory data analysis on this data set. Including: Identify the extent to which missing data is an issue Determine if the data types are appropriate for analysis Using a correlation matrix, pairplot or alternative method, identify whether collinearity is present in the data Identify and discuss anything else interesting that you see in the data Prepare to build a linear regression model to explain the variation in annual_income_ppp using the other data in the data set. Are there any fields which you believe should not be included in the model? If so, why? Would you consider imputing missing data for some or all fields where it is an issue? If so, what might be some simple ways to impute the missing data? Which variables are categorical? Convert these variables to dummy variables using a convenient function or using your own approach. Run and interpret the model. For convenience, and to avoid long formula strings, you can use the formula notation annual_income_ppp ~ . which means ‘regress annual_income against everything else’. You can also remove fields this way, for example annual_income_ppp ~ . - family_size. Determine what variables are significant predictors of annual income and what is the effect of each on the outcome. Determine the overall fit of the model. Do some simple analysis on the residuals of the model to determine if the model is safe to interpret. Experiment with improving the model fit through possible interaction terms or non-linear extensions. Comment on your results. Did anything in the results surprise you? If so, what might be possible explanations for this. Explain why you would or would not be comfortable using a model like this in a predictive setting - for example to help employers determine the right pay for employees. "],
["binomial-logistic-regression-for-binary-outcomes.html", "Chapter 6 Binomial Logistic Regression for Binary Outcomes", " Chapter 6 Binomial Logistic Regression for Binary Outcomes In the previous chapter we looked at how to explain outcomes that have continuous scale, such as quantity, money, height or weight. While there are a number of typical outcomes of this type in the people analytics domain, they are not the most common form of outcomes that are typically modeled. Much more common are situations where the outcome of interest takes the form of a limited set of classes. Binary (two class) problems are very common. Hiring, promotion and attrition are often modeled as binary outcomes: for example ‘Promoted’ or ‘Not promoted’. Multi-class outcomes like performance ratings on an ordinal scale, or survey responses on a Likert scale are often converted to binary outcomes by dividing the ratings into two groups, for example ‘High’ and ‘Not High’. In any situation where our outcome is binary, we are effectively working with likelihoods. These are not generally linear in nature, and so we no longer have the comfort of our inputs being directly linearly related to our outcome. Therefore, direct linear regression methods such as Ordinary Least Squares regression are not well suited to outcomes of this type. Instead, linear relationships can be inferred on transformations of the outcome variable, which gives us a path to building interpretable models. Hence, binomial logistic regression is said to be in a class of generalized linear models or GLMs. Understanding logistic regression and using it reliably in practice is not straightforward, but it is an invaluable skill to have in the people analytics domain. The mathematics of this chapter is a little more involved, but worth the time investment in order to build a competent understanding of how to interpret these types of models. "],
["when-to-use-it.html", "6.1 When to use it", " 6.1 When to use it 6.1.1 Origins and intuition of binomial logistic regression The logistic function was introduced by the Belgian Mathematician Pierre François Verhulst in the mid-1800s as a tool for modeling population growth for humans, animals and certain species of plants and fruits. By this time, it was generally accepted that population growth could not continue exponentially forever, and that there were environmental and resource limits which place a maximum limit on the size of a population. The formula for Verhulst’s function was: \\[ y = \\frac{L}{1 + e^{-k(x - x_0)}} \\] where \\(e\\) is the exponential constant, \\(x_0\\) is the value of \\(x\\) at the midpoint, \\(L\\) is the maximum value of \\(y\\) (known as the ‘carrying capacity’) and \\(k\\) is the maximum gradient of the curve. The logistic function, as shown in Figure 6.1, was felt to accurately capture the theorized stages of population growths, with slower growth in the initial stage, moving to exponential growth during the intermediate stage and then to slower growth as the population approaches its carrying capacity. Figure 6.1: Verhulst’s Logistic Function modeled both the exponential nature and the natural limit of population expansion In the early 20th century, starting with applications in economics and in chemistry, the logistic function was adopted in a wide array of fields as a useful tool for modeling phenomena. In statistics, it was quickly noted that the logistic function has a similar S-shape (or sigmoid) to a cumulative normal distribution of probability, as depicted in Figure 6.213). However, the logistic function has a clearer mathematical formula which is easier to perform calculus on, and therefore easier to use to develop inferential models based on maximum likelihood. So statisticians started to observe that they could work with a more ‘plyable’ function that was very close in nature to a normal probability distribution. Unsurprisingly, therefore, the logistic model soon became a common approach to modeling probabilistic phenomena. Figure 6.2: The logistic function is very similar to a cumulative normal distribution, but easier to work with mathematically 6.1.2 Use cases for binomial logistic regression Binomial logistic regression can be used when the outcome of interest is binary or dichotomous in nature. That is, it takes one of two values. For example, one or zero, true or false, yes or no. These classes are commonly described as ‘positive’ and ‘negative’ classes. There is an underlying assumption that the cumulative probability of the outcome takes a shape similar to a cumulative normal distribution. Here are some example questions that could be approached using binomial logistic regression: Given a set of data about sales managers in an organization, including performance against targets, team size, tenure in the organization and other factors, what impact do these factors have on the likelihood of the individual receiving a high performance rating? Given a set of demographic, income and location data, what influence does each have on the likelihood of an individual voting in an election? Given a set of statistics about the in-game activity of soccer players, what relationship does each statistic have with the likelihood of a player scoring a goal? 6.1.3 Walkthrough example You are an analyst for a large company consisting of regional sales teams across the country. Twice every year, this company promotes some of its salespeople. Promotion is at the discretion of the head of each regional sales team, taking into consideration financial performance, customer satisfaction ratings, recent performance ratings and personal judgment. You are asked by the management of the company to conduct an analysis to determine how the factors of financial performance, customer ratings and performance ratings influence the likelihood of a given salesperson being promoted. You are provided with a dataset here, containing data for the last three years of salespeople considered for promotion. The data contains the following fields: promoted: A binary value indicating 1 if the individual was promoted and 0 if not. sales: the sales (in thousands of dollars) attributed to the individual in the period of the promotion customer_rate: the average satisfaction rating from a survey of the individuals customers during the promotion period performance: the most recent performance rating prior to promotion, from 1 (lowest) to 4 (highest) # obtain data from online csv at github url &lt;- &quot;https://raw.githubusercontent.com/keithmcnulty/eampa/master/data/salespeople.csv&quot; salespeople &lt;- read.csv(url) # look at the first few rows of data head(salespeople) ## promoted sales customer_rate performance ## 1 0 594 3.94 2 ## 2 0 446 4.06 3 ## 3 1 674 3.83 4 ## 4 0 525 3.62 2 ## 5 1 657 4.40 3 ## 6 1 918 4.54 2 The data looks as expected. Let’s get a summary of the data: summary(salespeople) ## promoted sales customer_rate performance ## Min. :0.0000 Min. :151.0 Min. :1.000 Min. :1.0 ## 1st Qu.:0.0000 1st Qu.:389.2 1st Qu.:3.000 1st Qu.:2.0 ## Median :0.0000 Median :475.0 Median :3.620 Median :3.0 ## Mean :0.3219 Mean :527.0 Mean :3.608 Mean :2.5 ## 3rd Qu.:1.0000 3rd Qu.:667.2 3rd Qu.:4.290 3rd Qu.:3.0 ## Max. :1.0000 Max. :945.0 Max. :5.000 Max. :4.0 ## NA&#39;s :1 NA&#39;s :1 NA&#39;s :1 First we see a small number of missing values, and we should remove those observations. We see that about a third of individuals were promoted, that sales ranged from $150k to $940k, that as expected the average satisfaction ratings range from 1 to 5, and finally we see four performance ratings, although the performance categories are numeric when they should be an ordered factor, and promoted is numeric when it should be categorical. Let’s convert these and then let’s do a pairplot to get a quick view on some possible underlying relationships (Figure 6.3). library(ggplot2) library(GGally) #remove NAs salespeople &lt;- salespeople[complete.cases(salespeople), ] #convert performance to ordered factor and promoted to categorical salespeople$performance &lt;- ordered(salespeople$performance, levels = 1:4) salespeople$promoted &lt;- as.factor(salespeople$promoted) # generate pairplot GGally::ggpairs(salespeople) Figure 6.3: Pairplot for the salespeople data set We can see from this pairplot that there are clearly higher sales for those who are promoted versus those who are not. We also see a moderate relationship between customer rating and sales, which is intuitive (if the customer doesn’t think much of you, sales wouldn’t likely be very high). So we can see that some relationships with our outcome may exist here, but it’s not clear how to tease them out and quantify them relative to each other. Let’s explore how binomial logistic regression can help us do this. The logistic function plotted in Figure 6.2 takes the simple form \\(y = \\frac{1}{1 + e^{-x}}\\)↩︎ "],
["modeling-probabilistic-outcomes-using-a-logistic-function.html", "6.2 Modeling probabilistic outcomes using a logistic function", " 6.2 Modeling probabilistic outcomes using a logistic function Imagine that you have an outcome \\(y\\) which either occurs or does not occur. The probability of \\(y\\) occurring, or \\(P(y = 1)\\), obviously takes a value between 0 and 1. Now imagine that some input variable \\(x\\) has a positive effect on the probability of the event taking place. Then you would naturally expect \\(P(y = 1)\\) to increase as \\(x\\) increases. In our salespeople data set, let’s plot our promotion outcome against the sales input. This can be seen in Figure 6.4. Figure 6.4: Plot of promotion against sales in salespeople data set It’s clear that the probability of promotion increases according to the sales level, with a lower limit of zero and an upper limit of 1. We could try to model this probability using our logistic function which we learned about in 6.1.1. For example, let’s plot the logistic function \\[ P(y = 1) = \\frac{1}{1 + e^{-k(x - x_{0})}} \\] on this data, where we set \\(x_0\\) to the mean of sales and \\(k\\) to be some maximum gradient value. In 6.5, we can see these logistic functions for different values of \\(k\\). All of these seem to reflect the pattern we are observing to some extent, but how do we determine the best fitting logistic function? Figure 6.5: Overlaying logistic functions with various gradients onto previous plot 6.2.1 Deriving the concept of log-odds Let’s look more carefully at the index of \\(e\\) in the denominator of our logistic function. Note that, because \\(x_{0}\\) is a constant, we have: \\[ -k(x - x_{0}) = - kx_{0} - kx = - (\\beta_{0} + \\beta_1x) \\] where \\(\\beta_0 = kx_0\\) and \\(\\beta_{1} = k\\). Therefore, \\[ P(y = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x)}} \\] Now we know that for any binary event \\(y\\), \\(P(y = 0)\\) is equal to \\(1 - P(y = 1)\\), so \\[ P(y = 0) = 1 - \\frac{1}{1 + e^{-(\\beta_1x + \\beta_0)}} \\] Putting these together, and using some manipulation, we find that \\[ \\frac{P(y = 1)}{P(y = 0)} = e^{\\beta_0 + \\beta_1x} \\] or alternatively, if we apply the natural logarithm to both sides \\[ \\ln\\left(\\frac{P(y = 1)}{P(y = 0)}\\right) = \\beta_0 + \\beta_1x \\] The right hand side should look familiar from the previous chapter on linear regression, meaning there is something here we can model linearly. But what is the left hand side? \\(P(y = 1)\\) is the probability that the event will occur, while \\(P(y = 0)\\) is the probability that the event will not occur. You may be familiar from sports like horse racing or other gambling situations that the ratio of these two represents the odds of an event. For example, if a given horse has odds of 1:4, this means that there is a 20% probability they will win and an 80% probability they will not14. Therefore we can conclude that the natural logarithm of the odds of \\(y\\) - usually termed the log odds of \\(y\\) - is linear in \\(x\\) and therefore we can model the log odds of \\(y\\) using similar linear regression methods to those studied in Chapter 515. 6.2.2 Modeling the log odds and interpreting the coefficients Let’s take our simple case of regressing sales against the outcome of promoted. We use a standard binomial GLM function and our standard formula notation which we learned in the previous chapter. # run a binomial model sales_model &lt;- glm(formula = promoted ~ sales, data = salespeople, family = &quot;binomial&quot;) # view the coefficients sales_model$coefficients ## (Intercept) sales ## -21.77642020 0.03675848 We can interpret the coefficients as follows: The (Intercept) coefficient is the value of the log odds with zero input value of \\(x\\). It is effectively the log odds of promotion if you made no sales. The sales coefficient represents the increase in the log odds of promotion associated with each unit increase in sales. We can convert these coefficients from log odds to odds by applying the exponent function, to return to the identity we had previously \\[ \\frac{P(y = 1)}{P(y = 0)} = e^{\\beta_0 + \\beta_1x} = e^{\\beta_0}(e^{\\beta_1})^x \\] From this, we can interpret that \\(e^{\\beta_0}\\) represents the base odds of promotion assuming no sales, and that for every additional unit sales, those base odds are multiplied by \\(e^{\\beta_1}\\). Given this multiplicative effect that \\(e^{\\beta_1}\\) has on the odds, it is known as an odds ratio. # convert log odds to base odds and odds ratio exp(sales_model$coefficients) ## (Intercept) sales ## 3.488357e-10 1.037442e+00 So we can see that the base odds of promotion with zero sales is very close to zero, which makes sense. Note that odds can only be precisely zero in a situation where it is impossible to be in the positive class (that is, nobody gets promoted). We can also see that each unit (that is, every $1000) of sales multiplies the base odds by approximately 1.04 - in other words it increases the odds of promotion by 4%. 6.2.3 Odds versus probability It is worth spending a little time understanding the concept of odds and how it relates to probability. It is extremely common for these two terms to be used synonymously, and this can lead to serious misunderstandings when interpreting a logistic regression model. If a certain event has a probability of 0.1, then this means that its odds are 1:9, or 0.111. If the probability is 0.5, then the odds are 1, if the probability is 0.9, then the odds are 9, and if the probability is 0.99, the odds are 99. As we approach a a probability of 1, the odds become exponentially large, as illustrated in Figure 6.6: Figure 6.6: Odds plotted against probability The consequences of this is that a given increase in odds can have very different effects on probability depending on what original probability was in the first place. If the probability was already quite low, for example 0.1, then a 4% increase in odds translates to odds of 0.116 which translates to a new probability of 0.103586, representing an increase in probability of 3.59%, which is very close to the increase in odds. If the probability was already high, say 0.9, then a 4% increase in odds translates to odds of 9.36, which translates to a new probability of 0.903475 representing an increase in probability of 0.39%, which is very different from the increase in odds. Figure 6.7 shows the impact of a 4% increase in odds according to the original probability of the event. Figure 6.7: Effect of 4% increase in odds plotted against original probability We can see that the closer the base probability is to zero, the similar the effect of the increase on both odds and on probability. However, the higher the probability of the event, the less impact the increase in odds has. In any case, it’s useful to remember the formulas for convertings odds to probability and vice-versa. If \\(O\\) represents odds and \\(P\\) represents probability then we have: \\[ O = \\frac{P}{1 - P} \\\\ P = \\frac{O}{1 + O} \\] Often in sports the odds are expressed in the reverse order but the concept is the same↩︎ In this case a more general form of the Ordinary Least Squares procedure is used to fit the model, known as maximum likelihood↩︎ "],
["running-a-multivariate-logistic-regression-model.html", "6.3 Running a multivariate logistic regression model", " 6.3 Running a multivariate logistic regression model The derivations in the previous section easily extend to multivariate data. Let \\(y\\) by a dichotomous outcome and let \\(x_1, x_2, ..., x_p\\) be our input variables. Then \\[ \\ln\\left(\\frac{P(y = 1)}{P(y = 0)}\\right) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p \\] for coefficients \\(\\beta_0, \\beta_1,..., \\beta_p\\). As before: \\(\\beta_0\\) represents the log odds of our outcome when all inputs are zero Each \\(\\beta_i\\) represents the increase in the log odds of our outcome associated with a unit change in \\(x_i\\), assuming no change in other inputs. Applying an exponent as before, we have \\[ \\begin{align*} \\frac{P(y = 1)}{P(y = 0)} &amp;= e^{\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p} \\\\ &amp;= e^{\\beta_0}(e^{\\beta_1})^{x_1}(e^{\\beta_2})^{x_2}...(e^{\\beta_p})^{x_p} \\end{align*} \\] Therefore we can conclude that: \\(e^{\\beta_0}\\) represents the odds of the outcome when all inputs are zero Each \\(e^{\\beta_i}\\) represents the odds ratio associated with a unit increase in \\(x_i\\) assuming no change in the other inputs (that is, a unit increase in \\(x_i\\) multiplies the odds of our outcome by \\(e^{\\beta_i}\\)). Let’s put this into practice. 6.3.1 Running and interpreting a multivariate logistic regression model Let’s use a binomial logistic regression model to understand how each of the three inputs in our salespeople data set influences the likelihood of promotion. First, as we learned previously, it is good practice to convert the categorical performance variable to a dummy variable16. library(dummies) # convert performance to dummy perf_dummies &lt;- dummies::dummy(&quot;performance&quot;, data = salespeople) # replace in salespeople dataframe salespeople_dummies &lt;- cbind(salespeople[c(&quot;promoted&quot;, &quot;sales&quot;, &quot;customer_rate&quot;)], perf_dummies) head(salespeople_dummies) ## promoted sales customer_rate performance1 performance2 performance3 performance4 ## 1 0 594 3.94 0 1 0 0 ## 2 0 446 4.06 0 0 1 0 ## 3 1 674 3.83 0 0 0 1 ## 4 0 525 3.62 0 1 0 0 ## 5 1 657 4.40 0 0 1 0 ## 6 1 918 4.54 0 1 0 0 Now we can run our model (using the formula promoted ~ . to mean regressing promoted against everything else) and view our coefficients. # run binomial glm full_model &lt;- glm(formula = &quot;promoted ~ .&quot;, family = &quot;binomial&quot;, data = salespeople_dummies) # get coefficient summary (coefs &lt;- summary(full_model)$coefficients) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -19.12443855 3.501115197 -5.46238483 4.697803e-08 ## sales 0.04012425 0.006576429 6.10122119 1.052611e-09 ## customer_rate -1.11213130 0.482681585 -2.30406822 2.121881e-02 ## performance1 -0.73449340 1.071963758 -0.68518492 4.932272e-01 ## performance2 -0.47149387 0.933503552 -0.50507989 6.135027e-01 ## performance3 -0.04953888 0.911614825 -0.05434189 9.566628e-01 Note how only three of the performance dummies have displayed. This is because everyone is in one of the four performance categories, so the model is using performance4 as the base case, so we can interpret each performance coefficient as the effect of a move to that performance category from performance4. We can already see from the P (&gt; |z|) column that only sales and customer_rate meet the significance threshold of less than 0.05. Interestingly, it appears from the Estimate column that customer_rate has a negative effect on the log odds of promotion. We can add an extra column to create the exponents of our estimated coefficients so that we can see the odds ratios. (full_coefs &lt;- cbind(coefs, odds_ratio = exp(full_model$coefficients))) ## Estimate Std. Error z value Pr(&gt;|z|) odds_ratio ## (Intercept) -19.12443855 3.501115197 -5.46238483 4.697803e-08 4.947227e-09 ## sales 0.04012425 0.006576429 6.10122119 1.052611e-09 1.040940e+00 ## customer_rate -1.11213130 0.482681585 -2.30406822 2.121881e-02 3.288573e-01 ## performance1 -0.73449340 1.071963758 -0.68518492 4.932272e-01 4.797484e-01 ## performance2 -0.47149387 0.933503552 -0.50507989 6.135027e-01 6.240693e-01 ## performance3 -0.04953888 0.911614825 -0.05434189 9.566628e-01 9.516682e-01 Now we can interpret our model as follows: Sales have a significant positive effect on the likelihood of promotion, with each additional thousand dollars of sales increasing the odds of promotion by 4%. Customer ratings have a significant negative effect on the likelihood of promotion. One full rating higher is associated with 67% lower odds of promotion. Performance ratings have no significant effect on the likelihood of promotion. The second conclusion may appear counterintuitive, but remember from our pairplot in 6.1.3 that there is already moderate correlation between sales and customer ratings, and this model will be controlling for that relationship. Recall that our odds ratios act assuming all other variables are the same. Therefore, if two individuals have the same sales and performance ratings, the one with the lower customer rating is more likely to have been promoted. Similarly, if two individuals have the same level of sales and the same customer rating, their performance rating will have no significant bearing on the likelihood of promotion. Many analysts will feel uncomfortable with stating these conclusions with too much precision, and therefore exponent confidence intervals can be calculated to provide a range for the odds ratios. exp(confint(full_model)) ## 2.5 % 97.5 % ## (Intercept) 1.505306e-12 1.750716e-06 ## sales 1.029762e+00 1.057214e+00 ## customer_rate 1.141645e-01 7.793018e-01 ## performance1 5.345231e-02 3.824309e+00 ## performance2 9.675452e-02 3.958066e+00 ## performance3 1.591405e-01 5.976988e+00 ## performance4 NA NA Therefore we can say that - all else being equal - every additional unit of sales increases the odds of promotion by between 3.0% and 5.7%, and every additional point in customer rating decreases the odds of promotion by between 22% and 89%. Similar to other regression models, the unit scale needs to be taken into consideration during interpretation. On first sight, a decrease of up to 89% in odds seems a lot more important than an increase of up to 5.7% in odds. However, the increase of up to 5.7% is for one unit ($1000) in many thousands of sales units, and over 10 or 100 additional units can have a substantial compound effect on odds of promotion. The decrease of up to 89% is on a full customer rating point on a scale of only 4 full points. 6.3.2 Understanding the fit and confidence of a binomial logistic regression model Understanding the fit of a binomial logistic regression model is not straightforward and sometimes controversial. Before we discuss this, let’s simplify our model based on our learning that the performance data has no significant effect on the outcome. ## simplify model simpler_model &lt;- glm(formula = promoted ~ sales + customer_rate, family = &quot;binomial&quot;, data = salespeople) As in the previous chapter, again we have the luxury of a three-dimensional model, so we can visualize it in Interactive Figure 6.8, revealing a 3D sigmoid curve which ‘twists’ to reflect the relative influence of sales and customer_rate on the outcome. Figure 6.8: 3D Visualization of the fitted simpler_model against the salespeople data No let’s look at the summary of our simpler_model. summary(simpler_model) ## ## Call: ## glm(formula = promoted ~ sales + customer_rate, family = &quot;binomial&quot;, ## data = salespeople) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.02984 -0.09256 -0.02070 0.00874 3.06380 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -19.517689 3.346762 -5.832 5.48e-09 *** ## sales 0.040389 0.006525 6.190 6.03e-10 *** ## customer_rate -1.122064 0.466958 -2.403 0.0163 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 440.303 on 349 degrees of freedom ## Residual deviance: 65.131 on 347 degrees of freedom ## AIC: 71.131 ## ## Number of Fisher Scoring iterations: 8 Note that our summary does not provide any of the statistics on overall model fit or confidence that we see in linear regression. The main reason for this is that there is no clear unified point of view in the statistics community on appropriate measures for these concepts in the case of logistic regression. Nevertheless, a number of options are available to modelers for estimating fit and confidence for these models. Pseudo-\\(R^2\\) measures are attempts to estimate the amount of variance in the outcome that is explained by the fitted model, analogous to the \\(R^2\\) in linear regression. There are numerous variants of pseudo-\\(R^2\\) with some of the most common listed here. McFadden’s \\(R^2\\) works by comparing the likelihood function of the fitted model with that of a random model and using this to estimate the explained variance in the outcome. Cox and Snell’s \\(R^2\\) works by applying a ‘sum of squares’ analogy to the likelihood functions to align more closely with the precise methodology for calculating \\(R^2\\) in linear regression. However, this usually means that the maximum value is less than 1, and in certain circumstances substantially less than 1, which can be problematic and unintuitive for an \\(R^2\\) Nagelkerke’s \\(R^2\\) resolves the issue with the upper bound for Cox and Snell by dividing Cox and Snell’s \\(R^2\\) by its upper bound. This restores an intuitive scale with a maximum of 1, but is considered somewhat arbitrary with limited theoretical foundation. Tjur’s \\(R^2\\) is a more recent and simpler concept. It is defined as simply the absolute difference between the predicted probabilities of the positive observations and those of the negative observations. Standard modeling functions generally do not offer the calculation of pseudo-\\(R^2\\) as standard, but numerous methods are available for their calculation. For example: library(DescTools) DescTools::PseudoR2(simpler_model, which = c(&quot;McFadden&quot;, &quot;CoxSnell&quot;, &quot;Nagelkerke&quot;, &quot;Tjur&quot;)) ## McFadden CoxSnell Nagelkerke Tjur ## 0.8520759 0.6576490 0.9187858 0.8784834 We see that the Cox and Snell variant is notably lower than the other estimates, which is consistent with the known issues with its upper bound. However, the other estimates are reasonably aligned and suggest a strong fit. Confidence or ‘goodness of fit’ tests for logistic regression models compare the predictions to the observed outcome and test the hypothesis that they are similar. This means that, unlike in linear regression, a low p-value indicates a poor fit. One commonly used method is the Hosmer-Lemeshow test, which divides the observations into a number of groups (usually ten) according to their fitted probabilities, calculates the proportion of each group that is positive and then compares this to the expected proportions based on the model prediction using a Chi-squared test. However, this method has limitations. It is particularly problematic for situations where there a low sample size and can return highly varied results based on the number of groups used. It is therefore recommended to use a range of goodness of fit tests, and not rely entirely on any one specific approach. In R, the LogisticDx package offers a range of diagnostic tools for logistic regression models, and is recommended for exploration. Here is an example for assessing goodness of fit. library(LogisticDx) # get range of goodness of fit diagnostics simpler_model_diagnostics &lt;- LogisticDx::gof(simpler_model, plotROC = FALSE) # returns a list names(simpler_model_diagnostics) ## [1] &quot;ct&quot; &quot;chiSq&quot; &quot;ctHL&quot; &quot;gof&quot; &quot;R2&quot; &quot;auc&quot; # in our case we are interested in goodness of fit statistics simpler_model_diagnostics$gof ## test stat val df pVal ## 1: HL chiSq 3.44576058 8 0.903358158 ## 2: mHL F 2.74709957 8 0.005971045 ## 3: OsRo Z -0.02415249 NA 0.980730971 ## 4: SstPgeq0.5 Z 0.88656856 NA 0.375311227 ## 5: SstPl0.5 Z 0.96819352 NA 0.332947728 ## 6: SstBoth chiSq 1.72340251 2 0.422442787 ## 7: SllPgeq0.5 chiSq 1.85473814 1 0.173233325 ## 8: SllPl0.5 chiSq 0.68570870 1 0.407627859 ## 9: SllBoth chiSq 1.86640617 2 0.393291943 This confirms that almost all tests, including the Hosmer-Lemeshow test which is the first in the list, suggest a fit for our model. Various measures of predictive accuracy can also be used to assess a binomial logistic regression model in a predictive context, such as precision, recall and ROC-curve analysis. These are particularly suited for implementations of logistic regression models as predictive classifiers in a Machine Learning context, a topic which is outside the scope of this book. However, a recommended source for a deeper treatment of goodness of fit tests for logistic regression models is David W. Hosmer Jr. (2013). 6.3.3 Model parsimony We saw that in both our linear regression and our logistic regression approach, we decided to drop variables from our model when we determined that they had no significant effect on the outcome. The principle of Occam’s Razor states that - all else being equal - the simplest explanation is the best. In this sense, a model that contains information that does not contribute to it primary inference objective is more complex than it needs to be. Such a model increases the communication burden in explaining its results to others, with no notable analytic benefit in return. Parsimony describes the concept of being careful with resources or with information. A model could be described as more parsimonious if it can achieve the same (or very close to the same) fit with a smaller number of inputs. The Aikake Information Criterion or AIC is a measure of model parsimony that is computed for log-likelihood models like logistic regression models, with a lower AIC indicating a more parsimonious model. AIC is often calculated as standard in summary reports of logistic regression models, but can also be calculated independently. Let’s compare the different iterations of our model in this chapter using AIC. # Sales only model AIC(sales_model) ## [1] 76.49508 # sales and customer rating model AIC(simpler_model) ## [1] 71.13145 # model with all inputs AIC(full_model) ## [1] 76.37433 We can see that the model which is limited to our two significant inputs - sales and customer rating - is determined to be the most parsimonious model according to the AIC. Note that the AIC should not be used to interpret model quality or confidence - it is possible that the lowest AIC might still be a very poor fit. Model parsimony becomes a substantial concern when there is a large number of input variables. As a general rule, the more input variables there are in a model the greater the chance that the model will be difficult to interpet clearly, and the greater the risk of measurement problems such as multicollinearity. Analysts who are eager to please their customers, clients, professors or bosses can easily be tempted to think up new potential inputs to their model, often derived mathematically from measures that are already inputs in the model. Before long the model is too complex and in extreme cases there are more inputs than there are observations. The primary way to to manage model complexity is to exercise caution in selecting model inputs. When large numbers of inputs are unavoidable, coefficient regularization methods such as LASSO regression can help with model parsimony. We will look at this briefly in a later chapter. References "],
["other-considerations-in-binomial-logistic-regression.html", "6.4 Other considerations in binomial logistic regression", " 6.4 Other considerations in binomial logistic regression To predict from new data, just use the predict() function as in the previous chapter. This function recognizes the type of model being used - in this case a generalized linear model - and adjusts its prediction approach accordingly. In particular, if you want to return the probability of the new observations being promoted, you need to use type = \"response\" as an argument. # define new observations (new_data &lt;- data.frame(sales = c(420, 510, 710), customer_rate = c(3.4, 2.3, 4.2))) ## sales customer_rate ## 1 420 3.4 ## 2 510 2.3 ## 3 710 4.2 #predict probability of promotion predict(simpler_model, new_data, type = &quot;response&quot;) ## 1 2 3 ## 0.00171007 0.18238565 0.98840506 Many of the principles covered in the previous chapter on linear regression are equally important in logistic regression. For example, input variables should be managed in a similar way. Collinearity and multicollinearity should be of concern. Interaction of input variables can be modeled. For the most part, analysts should be aware of the fundamental transformations which take place on the outcome \\(y\\) during logistic regression when they consider some of these issues (another reason to ensure that the mathematics covered earlier in this chapter is well understood). For example, while coefficients in linear regression have a direct additive impact on \\(y\\), in logistic regression they have a direct additive impact on the log odds of \\(y\\), or alternatively their exponents have a direct multiplicative impact on the odds of \\(y\\). Therefore multicollinear variables, as an example, risk introducing overestimated coefficients which inflate the log odds additively. The exponents of these overestimated coefficients will then inflate the odds multiplicatively. Because of the binary nature of our outcome variable, the residuals of a logistic regression model have limited direct application to the problem being studied. In practical contexts the residuals of logistic regression models are rarely examined, but they can be useful in identifying outliers or particularly influential observations and in assessing goodness of fit, as in 6.3.2. When residuals are examined, they need to be transformed in order to be analyzed appropriately. For example, the Pearson residual is a standardized form of residual from logistic regression which can be expected to have a normal distribution over large enough samples. We can see in Figure 6.9 that this is the case for our simpler_model, but that there are a small number of substantial underestimates in our model. A good source of further learning on diagnostics of logistic regression models is Menard (2010). d &lt;- density(residuals(simpler_model, &quot;pearson&quot;)) plot(d, main= &quot;&quot;) Figure 6.9: Distribution of Pearson residuals in simpler_model References "],
["learning-exercises-3.html", "6.5 Learning exercises", " 6.5 Learning exercises 6.5.1 Discussion questions Draw the shape of a logistic (sigmoid) function. Describe the three population growth phases it was originally intended to model. Explain why the logistic function is useful to statisticians in modeling. In the formula for the logistic function in Section 6.1.1, what might be a common value for \\(L\\) in probabilistic applications? Why? What types of problems are suitable for logistic regression modeling? Can you think of some modeling scenarios in your work or studies that could use a logistic regression approach? Explain the concept of odds. How do odds differ from probability? How do odds change as probability increases? Complete the following: If an event has a 1% probability of occurring, a 10% increase in odds results in an almost __% increase in probability If an event has a 99% probability of occurring, a 10% increase in odds results in an almost __% increase in probability Describe how the coefficients of a logistic regression model affect the fitted outcome. If \\(\\beta\\) is a coefficient estimate, how is the odds ratio associated with \\(\\beta\\) calculated and what does it mean? What are some of the options for determining how well a regression model fits? Describe the concept of model parsimony. What measure is commonly used to determine the most parsimonious logistic regression model? 6.5.2 Data exercises A nature preservation charity has asked you to analyze some data to help them understand the features of those members of the public who donated in a given month. The charity_donation data set can be found at https://raw.githubusercontent.com/keithmcnulty/eampa/master/data/charity_donation.csv. It contains the following data: n_donations: The total number of times the individual donated previous to the month being studied. total_donations: The total amount of money donated by the individual previous to the month being studied time_donating: The number of months between the first donation and the month being studied recent_donation: Whether or not the individual donated in the month being studied last_donation: The number of months between the most recent previous donation and the month beng studied gender: The gender of the individual reside: Whether the person resides in an Urban or Rural Domestic location or Overseas age: The age of the individual View the data and obtain statistical summaries. Ensure data types are appropriate and there is no missing data. Determine the outcome and input variables. Using a pairplot or by plotting or correlating selected fields, try to hypothesize which variables may be significant in explaining who recently donated. Run a binomial logistic regression model using all input fields. Determine which input variables have a significant effect on the outcome and the direction of that effect. Calculate the odds ratios for the significant variables and explain their impact on the outcome Check for collinearity or multicollinearity in your model using methods from previous chapters. Experiment with model parsimony by reducing input variables that do not have a significant impact on the outcome. Decide on the most parsimonious model. Calculate a variety of Pseudo-\\(R^2\\) variants for your model. How would you explain these to someone with no statistics expertise? Report the conclusions of your modeling exercise to the charity by writing a simple explanation that assumes no knowledge of statistics. Extension: Using a variety of methods of your choice, test the hypothesis that your model fits the data. How conclusive are your tests? "],
["multinomial-logistic-regression-for-nominal-category-outcomes.html", "Chapter 7 Multinomial Logistic Regression for Nominal Category Outcomes", " Chapter 7 Multinomial Logistic Regression for Nominal Category Outcomes In the previous chapter we looked at how to model a binary or dichotomous outcome using a logistic function. In this chapter we look at how to extend this to the case when the outcome has a number of categories that do not have any order to them. When an outcome has this nominal categorical form, it does not have a sense of direction. There is no ‘better’ or ‘worse’, no ‘higher’ or ‘lower’, there is only ‘different’. "],
["when-to-use-it-1.html", "7.1 When to use it", " 7.1 When to use it 7.1.1 Intuition for multinomial logistic regression In fact, a dichotomous outcome like we studied in the previous chapter is already a nominal outcome with two categories, so in principle we already have the basic technology with which to study this problem if the number of categories increases. That said, the way we approach the problem can differ according to the types of inferences we wish to make. If we only wish to make inferences about the choice of each specific category - what drives whether an observation is in Category A the others, or Category B versus the others - then we have the option of running separate binomial logistic regression models on a ‘one versus the others’ basis. In this case we can refine our model differently for each category, eliminating variables that are not significant in whether or not the observation is in that category. This could potentially lead to models being defined differently for different pairs of outcomes. This is sometimes called a stratified approach. If we wish, however, to make a more abstracted conclusion about what variables influence membership across all the categories, then we need to take a more holistic approach. While this would still be founded on binomial models, we would need to make the decisions on refining the model and interpreting the coefficients with all categories in mind. In this chapter we will briefly look at the stratified approach (which is effectively a repetition of work done in the previous chapter) before focusing more intently on how we construct models and make inferences using a multinomial approach. 7.1.2 Use cases for multinomial logistic regression Multinomial logistic regression is appropriate for any situation where a limited number of outcome categories (more than two) are being modeled and where those outcome categories have no order. An underlying assumption is the independence of irrelevant alternatives - one way of stating this is that adding an outcome option is expected to reduce the odds of the other options by an equal amount. In cases where this assumption is violated, one could choose to take a stratified approach, or attempt hierarchical or nested multinomial model alternatives. Examples of typical situations that might be modeled by multinomial logistic regression include: Modeling voting choice in elections with multiple candidates Modeling choice of career options by students Modeling choice of benefit options by employees 7.1.3 Walkthrough example You are an analyst at a large technology company. The company recently introduced a new healthcare provider for its employees. At the beginning of the year the employees had to choose one of three different healthcare plan products from this provider to best suit their needs. You have been asked to determine which factors influenced the choice in product. The data set can be found here and consists of: age: The age of the individual when they made the choice gender: The gender of the individual as stated when theu made the choice children: The number of child dependents the individual had at the time of the choice position_level: Position level in the company at the time they made the choice, where 1 is is the lowest and 5 is the highest tenure: Full years employed by the company at the time they made the choice First we load the data and take a look at it briefly: # load data from github url &lt;- &quot;https://raw.githubusercontent.com/keithmcnulty/eampa/master/data/health_insurance.csv&quot; health_insurance &lt;- read.csv(url) # view first few rows head(health_insurance) ## product age children position_level gender tenure ## 1 C 57 2 2 Male 10 ## 2 A 21 7 2 Male 7 ## 3 C 66 7 2 Male 1 ## 4 A 36 4 2 Female 6 ## 5 A 23 0 2 Male 11 ## 6 A 31 5 1 Male 14 # view structure str(health_insurance) ## &#39;data.frame&#39;: 1453 obs. of 6 variables: ## $ product : chr &quot;C&quot; &quot;A&quot; &quot;C&quot; &quot;A&quot; ... ## $ age : int 57 21 66 36 23 31 37 37 55 66 ... ## $ children : int 2 7 7 4 0 5 3 0 3 2 ... ## $ position_level: int 2 2 2 2 2 1 3 3 3 4 ... ## $ gender : chr &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Female&quot; ... ## $ tenure : int 10 7 1 6 11 14 12 25 3 18 ... It looks like two of these columns should be converted to factor - product and gender, so let’s do that and then run a pairplot for a quick overview of any patterns in Figure 7.1. library(ggplot2) library(GGally) # convert product and gender to factors health_insurance$product &lt;- as.factor(health_insurance$product) health_insurance$gender &lt;- as.factor(health_insurance$gender) GGally::ggpairs(health_insurance) Figure 7.1: Pairplot of the health_insurance data set It is immediately apparent that the data is somewhat chaotic here. However there are a few things to note. Firstly we notice that there is a relatively even spread in choice between the products. We also notice that age seems to be playing a role in product choice. There are also some mild-to-moderate correlations in the data - in particular between age and position_level, and between tenure and position_level. However, this problem is clearly more complex than we can determine from a bivariate perspective. "],
["stratified.html", "7.2 Running stratified binomial models", " 7.2 Running stratified binomial models 7.2.1 Modeling the choice of Product A versus other products One approach to this problem is to look at each product choice and treat it as an independent binomial logistic regression model, modeling that choice against all others. This means we can try to describe the dynamics of the choice of a specific product, but we have to be careful about conclusions we make about the choice between the three products as a whole. This approach would not be very efficient if we had a wider range of choices. However, since we only have three possible choices here, it is an option for us to take this approach. Let’s first create and refine a binomial model for the choice of Product A. library(dummies) # create dummies for product choice dummy_product &lt;- dummies::dummy(&quot;product&quot;, data = health_insurance) # combine to original set health_insurance &lt;- cbind(health_insurance, dummy_product) colnames(health_insurance) ## [1] &quot;product&quot; &quot;age&quot; &quot;children&quot; &quot;position_level&quot; &quot;gender&quot; &quot;tenure&quot; &quot;productA&quot; ## [8] &quot;productB&quot; &quot;productC&quot; # run a binomial model on all input variables (let glm() handle dummy variables) A_model &lt;- glm(formula = productA ~ age + gender + children + position_level + tenure, data = health_insurance, family = &quot;binomial&quot;) # summary summary(A_model) ## ## Call: ## glm(formula = productA ~ age + gender + children + position_level + ## tenure, family = &quot;binomial&quot;, data = health_insurance) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.19640 -0.43691 -0.07051 0.46304 2.37416 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 5.873634 0.453041 12.965 &lt; 2e-16 *** ## age -0.239814 0.013945 -17.197 &lt; 2e-16 *** ## genderMale 0.845978 0.168237 5.028 4.94e-07 *** ## genderNon-binary 0.222521 1.246591 0.179 0.858 ## children 0.240205 0.037358 6.430 1.28e-10 *** ## position_level 0.321497 0.071770 4.480 7.48e-06 *** ## tenure -0.003751 0.010753 -0.349 0.727 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1864.15 on 1452 degrees of freedom ## Residual deviance: 940.92 on 1446 degrees of freedom ## AIC: 954.92 ## ## Number of Fisher Scoring iterations: 6 We see that all variables except tenure seem to play a significant role in the choice of Product A, with older males more likely to choose other products, and those who are female, younger and have larger families more likely to choose product A. Based on this we can simplify our model to remove tenure, calculate odds ratios and we can also perform some model diagnostics if we wish, similar to how we approached the problem in the previous chapter. 7.2.2 Modeling other choices In a similar way we can produce two other models, representing the choice of Products B and C. Try to do this yourself, but you should conclude that these models produce similar significant variables, except that position_level does not appear to be significant in the choice of Product C. If we simplify all our three models we will have a differently defined model for the choice of Product C versus choice of the other products, but we can conclude in general that the only input variable that seems to be insignificant across all choices of product is tenure. These stratified results need to be interpreted carefully. For example, the odds ratios for the product based on a simplified model are as follows: C_simple &lt;- glm(formula = productC ~ age + children + gender, data = health_insurance) exp(C_simple$coefficients) ## (Intercept) age children genderMale genderNon-binary ## 0.5354420 1.0159741 1.0823520 1.1743936 0.9977781 The age odds ratio can be interpreted as a 1.6% increase in the odds of choosing Product C over all the others for every additional year in age. "],
["running-a-multinomial-regression-model.html", "7.3 Running a multinomial regression model", " 7.3 Running a multinomial regression model An alternative to running separate binary stratified models is to run a multinomial logistic regression model. A multinomial logistic model will base itself from a defined reference category, and run a linear model on the log-odds of membership of each of the other categories versus the reference category. Due to its extensive use in epidemiology and medicine, this is often known as the relative risk of one category compared to the reference category. Mathematically speaking. if \\(X\\) is the vector of input variables, and y takes the value \\(A\\), \\(B\\) or \\(C\\), with \\(A\\) as the reference, a multinomial logistic regression model will calculate: \\[ \\mathrm{ln}\\left(\\frac{P(y = B)}{P(y=A)}\\right) = \\alpha{X} \\] and \\[ \\mathrm{ln}\\left(\\frac{P(y = C)}{P(y=A)}\\right) = \\beta{X} \\] for different vectors of coefficients \\(\\alpha\\) and \\(\\beta\\). 7.3.1 Defining a reference level and running the model The nnet package in R contains a multinom() function for running a multinomial logistic regression model using neural network technology17. Before we can run the model we need to make sure our reference level is defined. # define reference by ensuring it is the first level of the factor health_insurance$product &lt;- relevel(health_insurance$product, ref = &quot;A&quot;) # check that A is now our reference levels(health_insurance$product) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; Once the reference outcome is defined, the multinom() function from the nnet package will run a series of binomial models comparing the reference to the alternative. library(nnet) multi_model &lt;- nnet::multinom(formula = product ~ age + gender + children + position_level + tenure, data = health_insurance) ## # weights: 24 (14 variable) ## initial value 1596.283655 ## iter 10 value 969.042921 ## iter 20 value 744.786124 ## final value 744.682377 ## converged summary(multi_model) ## Call: ## nnet::multinom(formula = product ~ age + gender + children + ## position_level + tenure, data = health_insurance) ## ## Coefficients: ## (Intercept) age genderMale genderNon-binary children position_level tenure ## B -4.60100 0.2436645 -2.38259765 0.2523409 -0.9677237 -0.4153040 0.011676034 ## C -10.22617 0.2698141 0.09670752 -1.2715643 0.2043568 -0.2135843 0.003263631 ## ## Std. Errors: ## (Intercept) age genderMale genderNon-binary children position_level tenure ## B 0.5105532 0.01543139 0.2324262 1.226141 0.06943089 0.08916739 0.01298141 ## C 0.6197408 0.01567034 0.1954353 2.036273 0.04960655 0.08226087 0.01241814 ## ## Residual Deviance: 1489.365 ## AIC: 1517.365 Notice that the output of multi_model is much less detailed than for our binomial models, and it effectively just delivers the coefficients and standard errors of the two models against the reference. To determine whether specific input variables are significant we will need to calculate the p-values of the coefficients manually by calculating the t-statistics and and converting (see 4.3.1). # calculate t-statistics of coefficients t_stats &lt;- summary(multi_model)$coefficients/summary(multi_model)$standard.errors # convert to p-values (p_values &lt;- (1 - pnorm(abs(t_stats)))*2) ## (Intercept) age genderMale genderNon-binary children position_level tenure ## B 0 0 0.0000000 0.8369465 0.000000e+00 3.199529e-06 0.3684170 ## C 0 0 0.6207192 0.5323278 3.796088e-05 9.419906e-03 0.7926958 7.3.2 Interpreting the model This confirms that all variables except tenure play a role in the choice between all products relative to a reference of Product A. We can also calculate odds ratios as before: exp(summary(multi_model)$coefficients) ## (Intercept) age genderMale genderNon-binary children position_level tenure ## B 1.004179e-02 1.275916 0.09231048 1.2870347 0.3799469 0.6601396 1.011744 ## C 3.621021e-05 1.309721 1.10153815 0.2803927 1.2267357 0.8076841 1.003269 Here are some examples of how these odds ratios can be interpreted in the multinomial context (used in combination with the p-values above): If Person 1 chose Product A and Person 2 the same as Person 1 in all aspects except older, then every additional year of age increases the odds that Person 2 will select Product B by approximately 28% and increases the odds that Person 2 will select Product C by approximately 31%. If Person 1 is Female and chose Product A and Person 2 is the same as Person 1 in all aspects except is Male, then the odds of Person 2 selecting Product B are reduced by 91%. If Person 1 chose Product A and Person 2 is the same as Person 1 in all aspects but has more children, then every additional child Person 2 has decreases the odds of Person 2 selecting product B by approximately 62% and increases the odds of Person 2 selecting Product C by approximately 23%. 7.3.3 Changing the reference It may be that a someone would like to hear the odds ratios stated against the reference of an individual choosing Product B. For example, what are the odds ratios of Product C relative to a reference of Product B? One way to do this would be to change the reference and run the model again. Another option is to note that: \\[ \\begin{align*} \\frac{P(y = C)}{P(y=B)} = \\frac{\\frac{P(y = C)}{P(y = A)}}{\\frac{P(y=B)}{P(y = A)}} = \\frac{e^{\\beta{X}}}{e^{\\alpha{X}}} = e^{(\\beta - \\alpha)X} \\end{align*} \\] Therefore \\[ \\mathrm{ln}\\left(\\frac{P(y = C)}{P(y=B)}\\right) = (\\beta - \\alpha)X \\] This means we can obtain the coefficients of C against the reference of B by simply calculating the difference of the coefficients of C and B against the common reference of A. Let’s do this. (coefs_c_to_b &lt;- summary(multi_model)$coefficients[2, ] - summary(multi_model)$coefficients[1, ]) ## (Intercept) age genderMale genderNon-binary children position_level tenure ## -5.625169520 0.026149597 2.479305168 -1.523905192 1.172080452 0.201719688 -0.008412403 If the number of categories in the outcome variable is limited, this can be an efficient way to obtain the model coefficients against various reference points without having to rerun models, and can easily be extended in a similar way to generate p-values and odds ratios. Neural networks are computational structures which consist of a network of nodes, each of which take an input and perform a mathematical function to return an output onwards in the network. Most commonly they are used in deep learning, but a simple neural network here can model these two different categories using a logistic function↩︎ "],
["model-simplification-fit-and-confidence-for-multinomial-logistic-regression-models.html", "7.4 Model simplification, fit and confidence for multinomial logistic regression models", " 7.4 Model simplification, fit and confidence for multinomial logistic regression models Simplifying a multinomial model needs to be done with care. In a binomial model, there is one set of coefficients and their p-values can be a strong guide to which variables can be removed safely. However, in multinomial models there are several sets of coefficients to consider. 7.4.1 Gradual safe elimination of variables In David W. Hosmer Jr. (2013), a gradual process of elimination of variables is recommended to ensure that variables that confound each other in the different logistic models are not accidentally dropped from the final model. The recommended approach is as follows: Start with the variable with the least significant p-values in all sets of coefficients - in our case tenure would be the obvious first candidate Run the multinomial model without this variable Test that none of the previous coefficients change by more than 20-25% If there was no such change, safely remove the variable and proceed to the next non-significant variable If there is such a change, retain the variable and proceed to the next non-significant variable Stop when all non-significant variables have been tested In our case, when we can compare the coeffiecients of the model with and without tenure and verify that the change is not substantial. # remove tenure simpler_multi_model &lt;- nnet::multinom(formula = product ~ age + gender + children + position_level, data = health_insurance) ## # weights: 21 (12 variable) ## initial value 1596.283655 ## iter 10 value 900.083750 ## iter 20 value 745.133360 ## final value 745.124043 ## converged #compare coefficients summary(multi_model)$coefficients ## (Intercept) age genderMale genderNon-binary children position_level tenure ## B -4.60100 0.2436645 -2.38259765 0.2523409 -0.9677237 -0.4153040 0.011676034 ## C -10.22617 0.2698141 0.09670752 -1.2715643 0.2043568 -0.2135843 0.003263631 summary(simpler_multi_model)$coefficients ## (Intercept) age genderMale genderNon-binary children position_level ## B -4.50090 0.2433855 -2.37713424 0.1712091 -0.9641956 -0.3912014 ## C -10.19269 0.2697629 0.09801281 -1.2963678 0.2051081 -0.2090884 We can see that only genderNon-binary changed significantly, but we note that this is on an extremely small sample size and so will not have any effect on our model18. It therefore appears safe to remove tenure. Furthermore, the Aikake Information Criterion is equally valid in multinomial models for evaluating model parsimony. Here we can calculate the AIC of our model with and without tenure is 1517.36 and 1514.25 respectively, confirming that the model without tenure is marginally more parsimonious. 7.4.2 Model fit and confidence Due to the nature of multinomial models with more than one set of coefficients, assessing fit and goodness of fit is more challenging, and is still an area of intense research. The most approachable method to assess model confidence is the Hosmer-Lemeshow test mentioned in the previous chapter, which was extended in Morten W. Fagerland (2008) for multinomial models. An implementation is available in the generalhoslem package in R. However, the Hosmer-Lemeshow test is not valid for models with a small number of input variables (fewer than ten), and therefore we will not experiment with it here. For further exploration of this topic, Chapter 8 of David W. Hosmer Jr. (2013) is recommended, and for a more thorough treatment of the entire topic of categorical analytics, Agresti (2007) is an excellent companion. References "],
["exercises.html", "7.5 Exercises", " 7.5 Exercises 7.5.1 Discussion questions Describe the difference between a stratified versus a multinomial approach to modeling an outcome with more than two nominal categories. Describe how you would interpret the odds ratio of an input variable for a given category in a stratified modeling approach. Define the reference of a multinomial logistic regression model with at least three nominal outcome categories. Describe how you would interpret the odds ratio of an input variable for a given category in a multinomial modeling approach. Given a multinomial logistic regression model with reference category A and outcome categories A, B, C and D, describe two ways to determine the coefficients of a multinomial logistic regression model with reference category C. Describe a process for safely simplifying a multinomial logistic regression model by removing input variables. 7.5.2 Data exercises Use the same health_insurance data set from this chapter to answer these questions . Complete the full stratified approach to modeling the three product choices that was started in 7.2. Calculate the coefficients, odds ratios and p-values in each case. Carefully write down your interpretation of the odds ratios from the previous question. Run a multinomial logistic regression model on the product outcome using Product B as reference. Calculate the coefficients, ratios and p-values in each case. Verify that the coefficients for Product C against reference Product B matches those calculated in 7.3.3 Carefully write down your interpretation of the odds ratios calculated in the previous question. Use the process described in 7.4.1 to simplify the multinomial model in Question 4. "],
["references.html", "Chapter 8 References", " Chapter 8 References "]
]
