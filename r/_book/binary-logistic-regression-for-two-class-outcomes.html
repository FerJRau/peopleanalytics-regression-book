<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Binary Logistic Regression for Two-Class Outcomes | Elementary and Advanced Methods in People Analytics</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Binary Logistic Regression for Two-Class Outcomes | Elementary and Advanced Methods in People Analytics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="keithmcnulty/eampa" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Binary Logistic Regression for Two-Class Outcomes | Elementary and Advanced Methods in People Analytics" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Keith McNulty" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-reg-ols.html"/>
<link rel="next" href="methods.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Prerequisites</a></li>
<li class="chapter" data-level="2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html"><i class="fa fa-check"></i><b>2</b> Elementary Linear Regression for Continuous Outcomes</a><ul>
<li class="chapter" data-level="2.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#when-ols"><i class="fa fa-check"></i><b>2.1</b> When to use it</a><ul>
<li class="chapter" data-level="2.1.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#origins-ols"><i class="fa fa-check"></i><b>2.1.1</b> Origins and Intuition of Linear Regression</a></li>
<li class="chapter" data-level="2.1.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#use-cases-ols"><i class="fa fa-check"></i><b>2.1.2</b> Use cases for Linear Regression</a></li>
<li class="chapter" data-level="2.1.3" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#walkthrough-ols"><i class="fa fa-check"></i><b>2.1.3</b> Walkthrough Example</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#simple-ols"><i class="fa fa-check"></i><b>2.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="2.2.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#linear-single"><i class="fa fa-check"></i><b>2.2.1</b> Linear relationship between a single input and an outcome</a></li>
<li class="chapter" data-level="2.2.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#minimising-error-ols"><i class="fa fa-check"></i><b>2.2.2</b> Minimising the error</a></li>
<li class="chapter" data-level="2.2.3" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#best-fit-simple-ols"><i class="fa fa-check"></i><b>2.2.3</b> Determining the best fit</a></li>
<li class="chapter" data-level="2.2.4" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#measuring-goodness-of-fit"><i class="fa fa-check"></i><b>2.2.4</b> Measuring ‘goodness of fit’</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#multiple-linear-regression"><i class="fa fa-check"></i><b>2.3</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="2.3.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#running-a-multiple-linear-regression-model-and-interpreting-its-coefficients"><i class="fa fa-check"></i><b>2.3.1</b> Running a multiple linear regression model and interpreting its coefficients</a></li>
<li class="chapter" data-level="2.3.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#coefficient-confidence"><i class="fa fa-check"></i><b>2.3.2</b> Coefficient confidence</a></li>
<li class="chapter" data-level="2.3.3" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#model-confidence"><i class="fa fa-check"></i><b>2.3.3</b> Model confidence</a></li>
<li class="chapter" data-level="2.3.4" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#making-predictions-from-your-model"><i class="fa fa-check"></i><b>2.3.4</b> Making predictions from your model</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#managing-inputs-in-linear-regression"><i class="fa fa-check"></i><b>2.4</b> Managing inputs in linear regression</a><ul>
<li class="chapter" data-level="2.4.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#relevance-of-input-variables"><i class="fa fa-check"></i><b>2.4.1</b> Relevance of input variables</a></li>
<li class="chapter" data-level="2.4.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#sparseness-missingness-of-data"><i class="fa fa-check"></i><b>2.4.2</b> Sparseness (‘missingness’) of data</a></li>
<li class="chapter" data-level="2.4.3" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#transforming-categorical-inputs-to-dummy-variables"><i class="fa fa-check"></i><b>2.4.3</b> Transforming categorical inputs to dummy variables</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#important-watchouts-in-linear-regression"><i class="fa fa-check"></i><b>2.5</b> Important watchouts in linear regression</a><ul>
<li class="chapter" data-level="2.5.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#assumption-of-linearity-and-additivity"><i class="fa fa-check"></i><b>2.5.1</b> Assumption of linearity and additivity</a></li>
<li class="chapter" data-level="2.5.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#assumption-of-constant-error-variance"><i class="fa fa-check"></i><b>2.5.2</b> Assumption of constant error variance</a></li>
<li class="chapter" data-level="2.5.3" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#assumption-of-normally-distributed-errors"><i class="fa fa-check"></i><b>2.5.3</b> Assumption of normally distributed errors</a></li>
<li class="chapter" data-level="2.5.4" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#collinearity"><i class="fa fa-check"></i><b>2.5.4</b> Avoiding high collinearity and multicollinearity between input variables</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#extending-multiple-linear-regression-into-non-linear-contexts"><i class="fa fa-check"></i><b>2.6</b> Extending multiple linear regression into non-linear contexts</a><ul>
<li class="chapter" data-level="2.6.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#interactions-between-input-variables"><i class="fa fa-check"></i><b>2.6.1</b> Interactions between input variables</a></li>
<li class="chapter" data-level="2.6.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#non-linear-modeling"><i class="fa fa-check"></i><b>2.6.2</b> Non-linear modeling</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#learning-exercises"><i class="fa fa-check"></i><b>2.7</b> Learning exercises</a><ul>
<li class="chapter" data-level="2.7.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#discussion-questions"><i class="fa fa-check"></i><b>2.7.1</b> Discussion questions</a></li>
<li class="chapter" data-level="2.7.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#data-exercise"><i class="fa fa-check"></i><b>2.7.2</b> Data exercise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="binary-logistic-regression-for-two-class-outcomes.html"><a href="binary-logistic-regression-for-two-class-outcomes.html"><i class="fa fa-check"></i><b>3</b> Binary Logistic Regression for Two-Class Outcomes</a><ul>
<li class="chapter" data-level="3.1" data-path="binary-logistic-regression-for-two-class-outcomes.html"><a href="binary-logistic-regression-for-two-class-outcomes.html#when-to-use-it"><i class="fa fa-check"></i><b>3.1</b> When to use it</a><ul>
<li class="chapter" data-level="3.1.1" data-path="binary-logistic-regression-for-two-class-outcomes.html"><a href="binary-logistic-regression-for-two-class-outcomes.html#logistic-origins"><i class="fa fa-check"></i><b>3.1.1</b> Origins and intuition of binary logistic regression</a></li>
<li class="chapter" data-level="3.1.2" data-path="binary-logistic-regression-for-two-class-outcomes.html"><a href="binary-logistic-regression-for-two-class-outcomes.html#use-cases-for-binary-logistic-regression"><i class="fa fa-check"></i><b>3.1.2</b> Use cases for binary logistic regression</a></li>
<li class="chapter" data-level="3.1.3" data-path="binary-logistic-regression-for-two-class-outcomes.html"><a href="binary-logistic-regression-for-two-class-outcomes.html#walkthrough-example"><i class="fa fa-check"></i><b>3.1.3</b> Walkthrough example</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="binary-logistic-regression-for-two-class-outcomes.html"><a href="binary-logistic-regression-for-two-class-outcomes.html#modeling-probabilistic-outcomes-using-a-logistic-function"><i class="fa fa-check"></i><b>3.2</b> Modeling probabilistic outcomes using a logistic function</a><ul>
<li class="chapter" data-level="3.2.1" data-path="binary-logistic-regression-for-two-class-outcomes.html"><a href="binary-logistic-regression-for-two-class-outcomes.html#deriving-the-concept-of-log-odds"><i class="fa fa-check"></i><b>3.2.1</b> Deriving the concept of log-odds</a></li>
<li class="chapter" data-level="3.2.2" data-path="binary-logistic-regression-for-two-class-outcomes.html"><a href="binary-logistic-regression-for-two-class-outcomes.html#modeling-the-log-odds-and-interpreting-the-coefficients"><i class="fa fa-check"></i><b>3.2.2</b> Modeling the log odds and interpreting the coefficients</a></li>
<li class="chapter" data-level="3.2.3" data-path="binary-logistic-regression-for-two-class-outcomes.html"><a href="binary-logistic-regression-for-two-class-outcomes.html#odds-versus-probability"><i class="fa fa-check"></i><b>3.2.3</b> Odds versus probability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>4</b> Methods</a></li>
<li class="chapter" data-level="5" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>5</b> Applications</a><ul>
<li class="chapter" data-level="5.1" data-path="applications.html"><a href="applications.html#example-one"><i class="fa fa-check"></i><b>5.1</b> Example one</a></li>
<li class="chapter" data-level="5.2" data-path="applications.html"><a href="applications.html#example-two"><i class="fa fa-check"></i><b>5.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>6</b> Final Words</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Elementary and Advanced Methods in People Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="binary-logistic-regression-for-two-class-outcomes" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Binary Logistic Regression for Two-Class Outcomes</h1>
<p>In the previous chapter we looked at how to explain outcomes that have continuous scale, such as quantity, money, height or weights. While there are a number of typical problems of this type in the people analytics domain, they are not the most common form of outcomes that are typically modeled. Much more common are situations where the outcome of interest takes the form of a limited set of classes. Two-class (binary) problems are very common. In business contexts hiring, promotion and attrition are of often modeled as binary outcomes: for example ‘Promoted’ or ‘Not promoted’. Even multi-class outcomes like performance, where individuals can have multiple performance ratings on an ordinal scale, are often converted to binary outcomes by dividing the performance ratings into two groups, for example ‘High’ and ‘Not High’.</p>
<p>In any situation where our outcome is binary we are effectively working with probabilities. Probability distributions are not generally linear in nature, and so we are no longer have the comfort of our inputs being <em>directly</em> linearly related to our outcome. Therefore, direct linear regression methods such as Ordinary Least Squares regression are not well suited to outcomes of this type. That said, linear relationships can be inferred on <em>transformations</em> of the outcome variable, which gives us a path to building interpretable models. Hence, logistic regression is said to be in a class of <em>generalized linear models</em> or <em>GLMs</em>. Because of the transformations of the outcome variable, the steps to interpretation of a binary logistic regression model are a little more involved than in the previous chapter. Understanding logistic regression and using it reliably in practice is not straightforward, but it is an invaluable skill to have in the people analytics domain.</p>
<div id="when-to-use-it" class="section level2">
<h2><span class="header-section-number">3.1</span> When to use it</h2>
<div id="logistic-origins" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Origins and intuition of binary logistic regression</h3>
<p>The <em>logistic function</em> was introduced by the Belgian Mathematician Pierre François Verhulst in the mid-1800s as a tool for modeling population growth for humans, animals and certain species of plants and fruits. By this time, it was generally accepted that population growth could not continue exponentially forever, and that there were environmental and resource limits which place a maximum limit on the size of a population, called the ‘carrying capacity’. The formula for Verhulst’s function was:</p>
<p><span class="math display">\[
y = \frac{L}{1 + e^{-k(x - x_0)}}
\]</span>
where <span class="math inline">\(e\)</span> is the exponential constant, <span class="math inline">\(x_0\)</span> is the value of <span class="math inline">\(x\)</span> at the midpoint, <span class="math inline">\(L\)</span> is the maximum value of <span class="math inline">\(y\)</span> (the ‘carrying capacity’) and <span class="math inline">\(k\)</span> is the maximum gradient of the curve.</p>
<p>The logistic function, as shown in Figure <a href="binary-logistic-regression-for-two-class-outcomes.html#fig:logistic-function-verhulst">3.1</a>, was felt to accurately capture the theorized stages of population growths, with slower growth in the initial stage, moving to exponential growth during the intermediate stage and then to slower growth as the population approaches its carrying capacity.</p>
<div class="figure" style="text-align: center"><span id="fig:logistic-function-verhulst"></span>
<img src="www/02/logistic-curve.png" alt="Verhulst's Logistic Function modeled both the exponential nature and the natural limit of population expansion"  />
<p class="caption">
Figure 3.1: Verhulst’s Logistic Function modeled both the exponential nature and the natural limit of population expansion
</p>
</div>
<p>In the early 20th century, starting with applications in economics and in chemistry, the logistic function was adopted in a wide array of fields as a useful tool for modeling phenomena. In statistics, it was quickly noted that the logistic function has a similar S-shape (or <em>sigmoid</em>) to a cumulative normal distribution of probability, as depicted in Figure <a href="binary-logistic-regression-for-two-class-outcomes.html#fig:norm-log-curves">3.2</a><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>). However, the logistic function has a clear and simple mathematical formula which is easy to perform calculus on, and therefore easier to use to develop inferential models based on maximum likelihood. So statisticians started to observe that they could work with a more ‘plyable’ function that was very close in nature to a normal probability distribution. Unsurprisingly, the logistic model soon became a common approach to modeling probabilistic phenomena.</p>
<div class="figure" style="text-align: center"><span id="fig:norm-log-curves"></span>
<img src="bookdown-demo_files/figure-html/norm-log-curves-1.png" alt="The logistic function is very similar to a cumulative normal distribution, but easier to work with mathematically" width="672" />
<p class="caption">
Figure 3.2: The logistic function is very similar to a cumulative normal distribution, but easier to work with mathematically
</p>
</div>
</div>
<div id="use-cases-for-binary-logistic-regression" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Use cases for binary logistic regression</h3>
<p>Binary logistic regression can be used in the following situation:</p>
<ol style="list-style-type: decimal">
<li>The outcome of interest is binary or dichotomous in nature. That is, it takes one of two values. For example, one or zero, true or false, yes or no. These classes are commonly described as ‘positive’ and ‘negative’ classes.</li>
<li>There is more than one input variable and you need to understand the <em>relative</em> impact of each variable on the likelihood of the output being in the positive class. If there is only one input variable, logistic regression can still be used but a Chi-squared test will produce similar results and is a generally simpler approach.</li>
</ol>
<p>Example questions that could be approached using binary logistic regression include:</p>
<ul>
<li>Given a set of data about sales managers in an organization, including performance against targets, team size, tenure in the organization and other factors, what impact do these factors have on the likelihood of the individual being a high performer?</li>
<li>Given a set of demographic, income and location data, what influence does each have on the likelihood of an individual voting in an election?</li>
<li>Given a set of responses to survey questions from a set of employees and data on the employees’ history with the organization, to what extent can the different survey responses explain the likelihood of an individual leaving the organization within a defined time period?</li>
</ul>
</div>
<div id="walkthrough-example" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Walkthrough example</h3>
<p>You are an analyst for a large company consisting of four regional sales teams across the country. Twice every year, this company promotes its salespeople. Promotion is at the discretion of the head of each regional sales team, taking into consideration financial performance, customer satisfaction ratings, recent performance ratings and personal judgment.</p>
<p>You are asked by the management of the company to conduct an analysis to determine how the factors of financial performance, customer ratings and performance ratings influence the likelihood of a given salesperson being promoted. You are provided with a dataset <a href="https://raw.githubusercontent.com/keithmcnulty/eampa/master/data/salespeople.csv">here</a>, containing data for the last three years of salespeople considered for promotion. The data contains the following fields:</p>
<ul>
<li><code>promoted</code>: A binary value indicating 1 if the individual was promoted and 0 if not.</li>
<li><code>sales</code>: the sales (in $000s) attributed to the individual in the period of the promotion</li>
<li><code>customer_rate</code>: the average satisfaction rating from a survey of the individuals customers during the promotion period</li>
<li><code>performance</code>: the most recent performance rating prior to promotion, from 1 (lowest) to 4 (highest)</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb1-1"></a><span class="co"># obtain data from online csv at github</span></span>
<span id="cb1-2"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb1-2"></a>url &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/keithmcnulty/eampa/master/data/salespeople.csv&quot;</span></span>
<span id="cb1-3"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb1-3"></a>salespeople &lt;-<span class="st"> </span><span class="kw">read.csv</span>(url)</span>
<span id="cb1-4"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb1-4"></a></span>
<span id="cb1-5"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb1-5"></a><span class="co"># look at the first few rows of data</span></span>
<span id="cb1-6"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb1-6"></a><span class="kw">head</span>(salespeople)</span></code></pre></div>
<pre><code>##   promoted sales customer_rate performance
## 1        0   594          3.94           2
## 2        0   446          4.06           3
## 3        1   674          3.83           4
## 4        0   525          3.62           2
## 5        1   657          4.40           3
## 6        1   918          4.54           2</code></pre>
<p>The data looks as expected. Let’s get a summary of the data:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb3-1"></a><span class="kw">summary</span>(salespeople)</span></code></pre></div>
<pre><code>##     promoted          sales       customer_rate    performance 
##  Min.   :0.0000   Min.   :151.0   Min.   :1.000   Min.   :1.0  
##  1st Qu.:0.0000   1st Qu.:389.2   1st Qu.:3.000   1st Qu.:2.0  
##  Median :0.0000   Median :475.0   Median :3.620   Median :3.0  
##  Mean   :0.3229   Mean   :527.0   Mean   :3.608   Mean   :2.5  
##  3rd Qu.:1.0000   3rd Qu.:667.2   3rd Qu.:4.290   3rd Qu.:3.0  
##  Max.   :1.0000   Max.   :945.0   Max.   :5.000   Max.   :4.0</code></pre>
<p>We see that about a third of individuals were promoted, that sales ranged from $150k to $940k, that as expected the satisfaction ratings range from 1 to 5, and finally we see four performance ratings, although the performance categories are numeric when they should be an ordered factor. Let’s convert them to an ordered factor and let’s do a pairplot to get a quick view on some underlying relationships:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb5-1"></a><span class="kw">library</span>(ggplot2)</span>
<span id="cb5-2"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb5-2"></a><span class="kw">library</span>(GGally)</span>
<span id="cb5-3"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb5-3"></a></span>
<span id="cb5-4"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb5-4"></a><span class="co">#convert performance to ordered factor</span></span>
<span id="cb5-5"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb5-5"></a>salespeople<span class="op">$</span>performance &lt;-<span class="st"> </span><span class="kw">ordered</span>(salespeople<span class="op">$</span>performance, <span class="dt">levels =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>)</span>
<span id="cb5-6"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb5-6"></a></span>
<span id="cb5-7"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb5-7"></a><span class="co"># generate pairplot</span></span>
<span id="cb5-8"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb5-8"></a>GGally<span class="op">::</span><span class="kw">ggpairs</span>(salespeople)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Here we are looking a the binary variable <code>promoted</code> on the x-axis in the first column. We can see that there are twice as many 0s as 1s in the first chart of that column, which confirms what we discovered in our summary. We can see from the distribution of the scatters for 0 and 1 in the rest of that column that there may be a relationship between sales and promotion and between customer rating and promotion. We can also see that none of those who received the bottom two performance ratings were promoted. We also see a moderate relationship between customer rating and sales, which is intuitive (if the customer doesn’t like you, sales wouldn’t likely be very high).</p>
<p>So we can see that some relationships with our outcome may exist here, but it’s not clear how to tease them out and quantify them relative to each other. Let’s explore how binary logistic regression can help us do this.</p>
</div>
</div>
<div id="modeling-probabilistic-outcomes-using-a-logistic-function" class="section level2">
<h2><span class="header-section-number">3.2</span> Modeling probabilistic outcomes using a logistic function</h2>
<p>Imagine that you have an outcome <span class="math inline">\(y\)</span> which either occurs or does not occur. The probability of <span class="math inline">\(y\)</span> occurring, or <span class="math inline">\(P(y = 1)\)</span>, obviously takes the a value between 0 and 1. Now imagine that some input variable <span class="math inline">\(x\)</span> has a positive effect on the probability of the event taking place. Then you would naturally expect <span class="math inline">\(P(y = 1)\)</span> to increase as <span class="math inline">\(x\)</span> increases.</p>
<p>In our <code>salespeople</code> data set, let’s plot our <code>promotion</code> outcome against the <code>sales</code> input. This can be seen in Figure <a href="binary-logistic-regression-for-two-class-outcomes.html#fig:prom-sales-plot">3.3</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:prom-sales-plot"></span>
<img src="bookdown-demo_files/figure-html/prom-sales-plot-1.png" alt="Plot of promotion against sales in `salespeople` data set" width="672" />
<p class="caption">
Figure 3.3: Plot of promotion against sales in <code>salespeople</code> data set
</p>
</div>
<p>It’s clear that the probability of promotion increases according to the sales level, with a lower limit of zero and an upper limit of 1. We could try to model this probability using our logistic function which we learned about in <a href="binary-logistic-regression-for-two-class-outcomes.html#logistic-origins">3.1.1</a>. For example, let’s plot the logistic function
<span class="math display">\[
P(y = 1) = \frac{1}{1 + e^{-k(x - x_{0})}}
\]</span></p>
<p>on this data, where we set <span class="math inline">\(x_0\)</span> to the the mean of <code>sales</code> and <span class="math inline">\(k\)</span> to be some gradient value. In <a href="binary-logistic-regression-for-two-class-outcomes.html#fig:prom-with-logistic">3.4</a>, we can see these logistic functions for gradients of one standard deviation of <code>sales</code>, half a standard deviation of <code>sales</code> and one quarter of a standard deviation of <code>sales</code>. All of these seem to reflect the pattern we are observing to some extent, but how do we determine the best fitting logistic function?</p>
<div class="figure" style="text-align: center"><span id="fig:prom-with-logistic"></span>
<img src="bookdown-demo_files/figure-html/prom-with-logistic-1.png" alt="Overlaying logistic functions with various gradients onto previous plot" width="672" />
<p class="caption">
Figure 3.4: Overlaying logistic functions with various gradients onto previous plot
</p>
</div>
<div id="deriving-the-concept-of-log-odds" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Deriving the concept of log-odds</h3>
<p>Let’s look more carefully at the index of <span class="math inline">\(e\)</span> in the denominator of our logistic function. Note that, because <span class="math inline">\(x_{0}\)</span> is a constant. we have:</p>
<p><span class="math display">\[
-k(x - x_{0}) = -kx - kx_{0} = - (\beta_{0} + \beta_1x)
\]</span>
for some values of <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>. Therefore,</p>
<p><span class="math display">\[
P(y = 1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1x)}}
\]</span></p>
<p>Now we know that for any binary event <span class="math inline">\(y\)</span>, <span class="math inline">\(P(y = 0) = 1 - P(y = 1)\)</span>, so</p>
<p><span class="math display">\[
P(y = 0) = 1 - \frac{1}{1 + e^{-(\beta_1x + \beta_0)}}
\]</span></p>
<p>Putting these together, and using some manipulation, we find that</p>
<p><span class="math display">\[
\frac{P(y = 1)}{P(y = 0)} = e^{\beta_0 + \beta_1x}
\]</span></p>
<p>or alternatively</p>
<p><span class="math display">\[
\log\left(\frac{P(y = 1)}{P(y = 0)}\right) = \beta_0 + \beta_1x
\]</span></p>
<p>The right hand side should look familiar from the previous chapter on linear regression, meaning there is something here we can model linearly. But what is the left hand side?</p>
<p><span class="math inline">\(P(y = 1)\)</span> is the probability that the event will occur, while <span class="math inline">\(P(y = 0)\)</span> is the probability that the event will not occur. You may be familiar from sports like horse racing or other gambling situations that the ratio of these two represents the <em>odds</em> of an event occurring. For example, if a given horse has odds of 1:4, this means that there is a 20% probability they will win and an 80% probability they will lose<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>.</p>
<p>Therefore we can conclude that the <em>log odds</em> of <span class="math inline">\(y\)</span> are linear in <span class="math inline">\(x\)</span> and therefore we can model the log odds of <span class="math inline">\(y\)</span> using similar linear regression methods to those studied in Chapter <a href="linear-reg-ols.html#linear-reg-ols">2</a><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>.</p>
</div>
<div id="modeling-the-log-odds-and-interpreting-the-coefficients" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Modeling the log odds and interpreting the coefficients</h3>
<p>Let’s take our simple case of regressing <code>sales</code> against the outcome of <code>promoted</code>. Although you would not usually choose to run a logistic regression model on a single output, we will do it here for illustration purposes before moving onto the multinomial case. We use a standard binomial GLM function and our standard formula notation which we learned in the previous chapter.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb6-1"></a><span class="co"># run a binomial model </span></span>
<span id="cb6-2"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb6-2"></a>simple_model &lt;-<span class="st"> </span><span class="kw">glm</span>(<span class="dt">formula =</span> promoted <span class="op">~</span><span class="st"> </span>sales, <span class="dt">data =</span> salespeople, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb6-3"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb6-3"></a></span>
<span id="cb6-4"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb6-4"></a><span class="co"># view the coefficients</span></span>
<span id="cb6-5"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb6-5"></a>simple_model<span class="op">$</span>coefficients</span></code></pre></div>
<pre><code>##  (Intercept)        sales 
## -21.77642020   0.03675848</code></pre>
<p>We can interpret the coefficients as follows:</p>
<ol style="list-style-type: decimal">
<li><p>The <code>(Intercept)</code> coefficient is the value of the log odds with zero input value of <span class="math inline">\(x\)</span>. It is effectively the log odds of promotion if you made no sales.</p></li>
<li><p>The <code>sales</code> coefficient represents the increase in the log odds of promotion associated with each unit increase in sales.</p></li>
</ol>
<p>We can convert these coefficients from log odds to simple odds by applying the exponent function, to return to the identity we had previously</p>
<p><span class="math display">\[
\frac{P(y = 1)}{P(y = 0)} = e^{\beta_0 + \beta_1x} = e^{\beta_0}(e^{\beta_1})^x
\]</span></p>
<p>From this, we can interpret that <span class="math inline">\(e^{\beta_0}\)</span> represents the base odds of promotion assuming no sales, and that for every additional unit sales, those base odds are multiplied by <span class="math inline">\(e^{\beta_1}\)</span>. Given this multiplicative effect that <span class="math inline">\(e^{\beta_1}\)</span> has on the odds, it is known as an <em>odds ratio</em>.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb8-1"></a><span class="co"># convert log odds to base odds and odds ratio</span></span>
<span id="cb8-2"><a href="binary-logistic-regression-for-two-class-outcomes.html#cb8-2"></a><span class="kw">exp</span>(simple_model<span class="op">$</span>coefficients)</span></code></pre></div>
<pre><code>##  (Intercept)        sales 
## 3.488357e-10 1.037442e+00</code></pre>
<p>So, we can see that the base odds of promotion with zero sales, is very close to zero, which makes sense. Note that odds can only be precisely zero in a situation where it is impossible to be in the positive class (eg nobody gets promoted). We can also see that each unit (that is, every $1000) of sales multiplies the base odds by approximately 1.04 - in other words it increases the odds of promotion by 4%.</p>
</div>
<div id="odds-versus-probability" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Odds versus probability</h3>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>The logistic function plotted in Figure <a href="binary-logistic-regression-for-two-class-outcomes.html#fig:norm-log-curves">3.2</a> takes the simple form <span class="math inline">\(y = \frac{1}{1 + e^{-x}}\)</span><a href="binary-logistic-regression-for-two-class-outcomes.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Often they are expressed in the reverse order but the concept is the same<a href="binary-logistic-regression-for-two-class-outcomes.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>In this case a more general form of the Ordinary Least Squares procedure is used to fit the model, known as <em>maximum likelihood</em>.<a href="binary-logistic-regression-for-two-class-outcomes.html#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-reg-ols.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/02-literature.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
