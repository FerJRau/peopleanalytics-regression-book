[
["binomial-logistic-regression-for-binary-outcomes.html", "Chapter 3 Binomial Logistic Regression for Binary Outcomes 3.1 When to use it 3.2 Modeling probabilistic outcomes using a logistic function 3.3 Running a multivariate logistic regression model 3.4 Other considerations in binomial logistic regression 3.5 Learning exercises", " Chapter 3 Binomial Logistic Regression for Binary Outcomes In the previous chapter we looked at how to explain outcomes that have continuous scale, such as quantity, money, height or weights. While there are a number of typical problems of this type in the people analytics domain, they are not the most common form of outcomes that are typically modeled. Much more common are situations where the outcome of interest takes the form of a limited set of classes. Two-class (binary) problems are very common. In business contexts hiring, promotion and attrition are of often modeled as binary outcomes: for example ‘Promoted’ or ‘Not promoted’. Even multi-class outcomes like performance, where individuals can have multiple performance ratings on an ordinal scale, are often converted to binary outcomes by dividing the performance ratings into two groups, for example ‘High’ and ‘Not High’. In any situation where our outcome is binary we are effectively working with probabilities. Probability distributions are not generally linear in nature, and so we no longer have the comfort of our inputs being directly linearly related to our outcome. Therefore, direct linear regression methods such as Ordinary Least Squares regression are not well suited to outcomes of this type. That said, linear relationships can be inferred on transformations of the outcome variable, which gives us a path to building interpretable models. Hence, binomial logistic regression is said to be in a class of generalized linear models or GLMs. Because of the transformations of the outcome variable, the steps to interpretation of a binomial logistic regression model are a little more involved than in the previous chapter. Understanding logistic regression and using it reliably in practice is not straightforward, but it is an invaluable skill to have in the people analytics domain. The mathematics of this chapter is a little more involved, but worth the time investment in order to build a competent understanding of how to interpret these types of models. 3.1 When to use it 3.1.1 Origins and intuition of binomial logistic regression The logistic function was introduced by the Belgian Mathematician Pierre François Verhulst in the mid-1800s as a tool for modeling population growth for humans, animals and certain species of plants and fruits. By this time, it was generally accepted that population growth could not continue exponentially forever, and that there were environmental and resource limits which place a maximum limit on the size of a population, called the ‘carrying capacity’. The formula for Verhulst’s function was: \\[ y = \\frac{L}{1 + e^{-k(x - x_0)}} \\] where \\(e\\) is the exponential constant, \\(x_0\\) is the value of \\(x\\) at the midpoint, \\(L\\) is the maximum value of \\(y\\) (the ‘carrying capacity’) and \\(k\\) is the maximum gradient of the curve. The logistic function, as shown in Figure 3.1, was felt to accurately capture the theorized stages of population growths, with slower growth in the initial stage, moving to exponential growth during the intermediate stage and then to slower growth as the population approaches its carrying capacity. Figure 3.1: Verhulst’s Logistic Function modeled both the exponential nature and the natural limit of population expansion In the early 20th century, starting with applications in economics and in chemistry, the logistic function was adopted in a wide array of fields as a useful tool for modeling phenomena. In statistics, it was quickly noted that the logistic function has a similar S-shape (or sigmoid) to a cumulative normal distribution of probability, as depicted in Figure 3.21). However, the logistic function has a clear and simple mathematical formula which is easy to perform calculus on, and therefore easier to use to develop inferential models based on maximum likelihood. So statisticians started to observe that they could work with a more ‘plyable’ function that was very close in nature to a normal probability distribution. Unsurprisingly, the logistic model soon became a common approach to modeling probabilistic phenomena. Figure 3.2: The logistic function is very similar to a cumulative normal distribution, but easier to work with mathematically 3.1.2 Use cases for binomial logistic regression Binomial logistic regression can be used in the following situation: The outcome of interest is binary or dichotomous in nature. That is, it takes one of two values. For example, one or zero, true or false, yes or no. These classes are commonly described as ‘positive’ and ‘negative’ classes. There is more than one input variable and you need to understand the relative impact of each variable on the likelihood of the output being in the positive class. If there is only one input variable, logistic regression can still be used but a Chi-squared test will produce similar results and is a generally simpler approach. Example questions that could be approached using binomial logistic regression include: Given a set of data about sales managers in an organization, including performance against targets, team size, tenure in the organization and other factors, what impact do these factors have on the likelihood of the individual being a high performer? Given a set of demographic, income and location data, what influence does each have on the likelihood of an individual voting in an election? Given a set of responses to survey questions from a set of employees and data on the employees’ history with the organization, to what extent can the different survey responses explain the likelihood of an individual leaving the organization within a defined time period? 3.1.3 Walkthrough example You are an analyst for a large company consisting of four regional sales teams across the country. Twice every year, this company promotes its salespeople. Promotion is at the discretion of the head of each regional sales team, taking into consideration financial performance, customer satisfaction ratings, recent performance ratings and personal judgment. You are asked by the management of the company to conduct an analysis to determine how the factors of financial performance, customer ratings and performance ratings influence the likelihood of a given salesperson being promoted. You are provided with a dataset here, containing data for the last three years of salespeople considered for promotion. The data contains the following fields: promoted: A binary value indicating 1 if the individual was promoted and 0 if not. sales: the sales (in $000s) attributed to the individual in the period of the promotion customer_rate: the average satisfaction rating from a survey of the individuals customers during the promotion period performance: the most recent performance rating prior to promotion, from 1 (lowest) to 4 (highest) # obtain data from online csv at github url &lt;- &quot;https://raw.githubusercontent.com/keithmcnulty/eampa/master/data/salespeople.csv&quot; salespeople &lt;- read.csv(url) # look at the first few rows of data head(salespeople) ## promoted sales customer_rate performance ## 1 0 594 3.94 2 ## 2 0 446 4.06 3 ## 3 1 674 3.83 4 ## 4 0 525 3.62 2 ## 5 1 657 4.40 3 ## 6 1 918 4.54 2 The data looks as expected. Let’s get a summary of the data: summary(salespeople) ## promoted sales customer_rate performance ## Min. :0.0000 Min. :151.0 Min. :1.000 Min. :1.0 ## 1st Qu.:0.0000 1st Qu.:389.2 1st Qu.:3.000 1st Qu.:2.0 ## Median :0.0000 Median :475.0 Median :3.620 Median :3.0 ## Mean :0.3229 Mean :527.0 Mean :3.608 Mean :2.5 ## 3rd Qu.:1.0000 3rd Qu.:667.2 3rd Qu.:4.290 3rd Qu.:3.0 ## Max. :1.0000 Max. :945.0 Max. :5.000 Max. :4.0 We see that about a third of individuals were promoted, that sales ranged from $150k to $940k, that as expected the satisfaction ratings range from 1 to 5, and finally we see four performance ratings, although the performance categories are numeric when they should be an ordered factor, and promoted is numeric when it should be categorical. Let’s convert these and then let’s do a pairplot to get a quick view on some underlying relationships: library(ggplot2) library(GGally) #convert performance to ordered factor and promoted to categorical salespeople$performance &lt;- ordered(salespeople$performance, levels = 1:4) salespeople$promoted &lt;- as.factor(salespeople$promoted) # generate pairplot GGally::ggpairs(salespeople) We can see from this pairplot that there are clearly higher sales for those who are promoted versus those who are not. We also see a moderate relationship between customer rating and sales, which is intuitive (if the customer doesn’t like you, sales wouldn’t likely be very high). So we can see that some relationships with our outcome may exist here, but it’s not clear how to tease them out and quantify them relative to each other. Let’s explore how binomial logistic regression can help us do this. 3.2 Modeling probabilistic outcomes using a logistic function Imagine that you have an outcome \\(y\\) which either occurs or does not occur. The probability of \\(y\\) occurring, or \\(P(y = 1)\\), obviously takes the a value between 0 and 1. Now imagine that some input variable \\(x\\) has a positive effect on the probability of the event taking place. Then you would naturally expect \\(P(y = 1)\\) to increase as \\(x\\) increases. In our salespeople data set, let’s plot our promotion outcome against the sales input. This can be seen in Figure 3.3. Figure 3.3: Plot of promotion against sales in salespeople data set It’s clear that the probability of promotion increases according to the sales level, with a lower limit of zero and an upper limit of 1. We could try to model this probability using our logistic function which we learned about in 3.1.1. For example, let’s plot the logistic function \\[ P(y = 1) = \\frac{1}{1 + e^{-k(x - x_{0})}} \\] on this data, where we set \\(x_0\\) to the the mean of sales and \\(k\\) to be some gradient value. In 3.4, we can see these logistic functions for gradients of one standard deviation of sales, half a standard deviation of sales and one quarter of a standard deviation of sales. All of these seem to reflect the pattern we are observing to some extent, but how do we determine the best fitting logistic function? Figure 3.4: Overlaying logistic functions with various gradients onto previous plot 3.2.1 Deriving the concept of log-odds Let’s look more carefully at the index of \\(e\\) in the denominator of our logistic function. Note that, because \\(x_{0}\\) is a constant. we have: \\[ -k(x - x_{0}) = -kx - kx_{0} = - (\\beta_{0} + \\beta_1x) \\] for some values of \\(\\beta_{0}\\) and \\(\\beta_{1}\\). Therefore, \\[ P(y = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x)}} \\] Now we know that for any binary event \\(y\\), \\(P(y = 0) = 1 - P(y = 1)\\), so \\[ P(y = 0) = 1 - \\frac{1}{1 + e^{-(\\beta_1x + \\beta_0)}} \\] Putting these together, and using some manipulation, we find that \\[ \\frac{P(y = 1)}{P(y = 0)} = e^{\\beta_0 + \\beta_1x} \\] or alternatively, if we apply the natural logarithm to both sides \\[ \\ln\\left(\\frac{P(y = 1)}{P(y = 0)}\\right) = \\beta_0 + \\beta_1x \\] The right hand side should look familiar from the previous chapter on linear regression, meaning there is something here we can model linearly. But what is the left hand side? \\(P(y = 1)\\) is the probability that the event will occur, while \\(P(y = 0)\\) is the probability that the event will not occur. You may be familiar from sports like horse racing or other gambling situations that the ratio of these two represents the odds of an event. For example, if a given horse has odds of 1:4, this means that there is a 20% probability they will win and an 80% probability they will not2. Therefore we can conclude that the natural logarithm of the odds of \\(y\\) - usually termed the log odds of \\(y\\) - is linear in \\(x\\) and therefore we can model the log odds of \\(y\\) using similar linear regression methods to those studied in Chapter 23. 3.2.2 Modeling the log odds and interpreting the coefficients Let’s take our simple case of regressing sales against the outcome of promoted. Although you would not usually choose to run a logistic regression model on a single output, we will do it here for illustration purposes before moving onto the multivariate case. We use a standard binomial GLM function and our standard formula notation which we learned in the previous chapter. # run a binomial model sales_model &lt;- glm(formula = promoted ~ sales, data = salespeople, family = &quot;binomial&quot;) # view the coefficients sales_model$coefficients ## (Intercept) sales ## -21.77642020 0.03675848 We can interpret the coefficients as follows: The (Intercept) coefficient is the value of the log odds with zero input value of \\(x\\). It is effectively the log odds of promotion if you made no sales. The sales coefficient represents the increase in the log odds of promotion associated with each unit increase in sales. We can convert these coefficients from log odds to simple odds by applying the exponent function, to return to the identity we had previously \\[ \\frac{P(y = 1)}{P(y = 0)} = e^{\\beta_0 + \\beta_1x} = e^{\\beta_0}(e^{\\beta_1})^x \\] From this, we can interpret that \\(e^{\\beta_0}\\) represents the base odds of promotion assuming no sales, and that for every additional unit sales, those base odds are multiplied by \\(e^{\\beta_1}\\). Given this multiplicative effect that \\(e^{\\beta_1}\\) has on the odds, it is known as an odds ratio. # convert log odds to base odds and odds ratio exp(sales_model$coefficients) ## (Intercept) sales ## 3.488357e-10 1.037442e+00 So, we can see that the base odds of promotion with zero sales, is very close to zero, which makes sense. Note that odds can only be precisely zero in a situation where it is impossible to be in the positive class (eg nobody gets promoted). We can also see that each unit (that is, every $1000) of sales multiplies the base odds by approximately 1.04 - in other words it increases the odds of promotion by 4%. 3.2.3 Odds versus probability It is worth spending a little time understanding the concept of odds and how it relates to probability. It is extremely common for these two terms to be used synonymously, and this can lead to serious misunderstandings when interpreting a logistic regression model. If a certain event has a probability of 0.1, then this means that its odds are 1:9, or 0.111. If the probability is 0.5, then the odds are 1, if the probability is 0.9, then the odds are 9, and if the probability is 0.99, the odds are 99. As we approach a a probability of 1, the odds become exponentially large, as illustrated in Figure 3.5: Figure 3.5: Odds plotted against probability The consequences of this is that a given increase in odds can have very different effects on probability depending on what original probability was in the first place. If the probability was already quite low, for example 0.1, then a 4% increase in odds translates to odds of 0.116 which translates to a new probability of 0.103586, representing an increase in probability of 3.59%, which is very close to the increase in odds. If the probability was already high, say 0.9, then a 4% increase in odds translates to odds of 9.36, which translates to a new probability of 0.903475 representing an increase in probability of 0.39%, which is very different from the increase in odds. Figure 3.6 shows the impact of a 4% increase in odds according to the original probability of the event. Figure 3.6: Effect of 4% increase in odds plotted against original probability We can see that the closer the base probability is to zero, the similar the effect of the increase on both odds and on probability. However, the higher the probability of the event, the less impact the increase in odds has. In any case, it’s useful to remember the formulas for convertings odds to probability and vice-versa. If \\(O\\) represents odds and \\(P\\) represents probability then we have: \\[ O = \\frac{P}{1 - P} \\\\ P = \\frac{O}{1 + O} \\] 3.3 Running a multivariate logistic regression model The derivations in the previous section easily extend to multivariate data. Let \\(y\\) by a dichotomous outcome and let \\(x_1, x_2, ..., x_p\\) be our input variables. Then \\[ \\ln\\left(\\frac{P(y = 1)}{P(y = 0)}\\right) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p \\] for coefficients \\(\\beta_0, \\beta_1,..., \\beta_p\\). As before: \\(\\beta_0\\) represents the log odds of our outcome when all inputs are zero Each \\(\\beta_i\\) represents the increase in the log odds of our outcome associated with a unit change in \\(x_i\\). Applying an exponent as before, we have \\[ \\begin{align*} \\frac{P(y = 1)}{P(y = 0)} &amp;= e^{\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p} \\\\ &amp;= e^{\\beta_0}(e^{\\beta_1})^{x_1}(e^{\\beta_2})^{x_2}...(e^{\\beta_p})^{x_p} \\end{align*} \\] Therefore we can conclude that: \\(e^{\\beta_0}\\) represents the odds of the outcome when all inputs are zero Each \\(e^{\\beta_i}\\) represents the odds ratio associated with a unit increase in \\(x_i\\) assuming no change in the other inputs (that is, a unit increase in \\(x_i\\) multiplies the odds of our outcome by \\(e^{\\beta_i}\\)). Let’s put this into practice. 3.3.1 Running and interpreting a multivariate logistic regression model Let’s use a binomial logistic regression model to understand how each of the three inputs in out salespeople data set influences the likelihood of promotion. First, as we learned previously, it is good practice to convert the categorical performance variable to a dummy variable4. library(dummies) # convert performance to dummy perf_dummies &lt;- dummies::dummy(&quot;performance&quot;, data = salespeople) # replace in salespeople dataframe salespeople_dummies &lt;- cbind(salespeople[c(&quot;promoted&quot;, &quot;sales&quot;, &quot;customer_rate&quot;)], perf_dummies) head(salespeople_dummies) ## promoted sales customer_rate performance1 performance2 performance3 ## 1 0 594 3.94 0 1 0 ## 2 0 446 4.06 0 0 1 ## 3 1 674 3.83 0 0 0 ## 4 0 525 3.62 0 1 0 ## 5 1 657 4.40 0 0 1 ## 6 1 918 4.54 0 1 0 ## performance4 ## 1 0 ## 2 0 ## 3 1 ## 4 0 ## 5 0 ## 6 0 Now we can run our model (using the formula promoted ~ . to mean regressing promoted against everything else) and view our coefficients. # run binomial glm full_model &lt;- glm(formula = &quot;promoted ~ .&quot;, family = &quot;binomial&quot;, data = salespeople_dummies) # get coefficient summary (coefs &lt;- summary(full_model)$coefficients) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -19.12443855 3.501115197 -5.46238483 4.697803e-08 ## sales 0.04012425 0.006576429 6.10122119 1.052611e-09 ## customer_rate -1.11213130 0.482681585 -2.30406822 2.121881e-02 ## performance1 -0.73449340 1.071963758 -0.68518492 4.932272e-01 ## performance2 -0.47149387 0.933503552 -0.50507989 6.135027e-01 ## performance3 -0.04953888 0.911614825 -0.05434189 9.566628e-01 Note how only three of the performance dummies have displayed. This is because the model is using performance4 as the base case, so we can interpret each performance coefficient as the effect of a move to that performance category from performance4. We can already see from the P (&gt; |z|) column that only sales and customer_rate meet the significance threshold of less than 0.05. Interestingly, it appears from the Estimate column that customer_rate has a negative effect on the log odds of promotion. We can add an extra column to create the exponents of our estimated coefficients so that we can see the odds ratios: (full_coefs &lt;- cbind(coefs, odds_ratio = exp(full_model$coefficients))) ## Estimate Std. Error z value Pr(&gt;|z|) odds_ratio ## (Intercept) -19.12443855 3.501115197 -5.46238483 4.697803e-08 4.947227e-09 ## sales 0.04012425 0.006576429 6.10122119 1.052611e-09 1.040940e+00 ## customer_rate -1.11213130 0.482681585 -2.30406822 2.121881e-02 3.288573e-01 ## performance1 -0.73449340 1.071963758 -0.68518492 4.932272e-01 4.797484e-01 ## performance2 -0.47149387 0.933503552 -0.50507989 6.135027e-01 6.240693e-01 ## performance3 -0.04953888 0.911614825 -0.05434189 9.566628e-01 9.516682e-01 Now we can interpret our model as follows: Sales have a significant positive effect on the likelihood of promotion, with each additional thousand dollars of sales increasing the odds of promotion by 4%. Customer ratings have a significant negative effect on the likelihood of promotion. One full rating higher is associated with 67% lower odds of promotion. Performance ratings have no significant effect on the likelihood of promotion. The second conclusion may appear counterintuitive, but remember from our pairplot in 3.1.3 that there is already moderate correlation between sales and customer ratings, and this model will be controlling for that relationship. Recall that our odds ratios act assuming all other variables are the same. Therefore, if two individuals have the same sales, the one with the lower customer rating is more likely to have been promoted. Similarly, if two individuals have the same level of sales and the same customer rating, their performance rating will have no significant bearing on the likelihood of promotion. Similar to other regression models, the unit scale needs to be taken into consideration during interpretation. On first sight, a decrease of 67% in odds seems a lot more important than an increase of 4% in odds. However, the increase of 4% is for one unit ($1000) in many thousands of sales units, and over 10 or 100 additional units can have a substantial compound effect on odds of promotion. The decrease of 67% is on a full customer rating point on a scale of only 4 full points. 3.3.2 Understanding the fit and confidence of a binomial logistic regression model Understanding the fit of a binomial logistic regression model is not straightforward and sometimes controversial. Before we discuss this, let’s simplify our model based on our learning that the performance data has no significant effect on the outcome. ## simplify model simpler_model &lt;- glm(formula = promoted ~ sales + customer_rate, family = &quot;binomial&quot;, data = salespeople) # view summary summary(simpler_model) ## ## Call: ## glm(formula = promoted ~ sales + customer_rate, family = &quot;binomial&quot;, ## data = salespeople) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.02984 -0.09256 -0.02070 0.00874 3.06380 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -19.517689 3.346762 -5.832 5.48e-09 *** ## sales 0.040389 0.006525 6.190 6.03e-10 *** ## customer_rate -1.122064 0.466958 -2.403 0.0163 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 440.303 on 349 degrees of freedom ## Residual deviance: 65.131 on 347 degrees of freedom ## AIC: 71.131 ## ## Number of Fisher Scoring iterations: 8 Note that our summary does not provide any of the statistics on overall model fit or confidence that we see in linear regression. The main reason for this is that there is no clear unified point of view in the statistics community on appropriate measures for these concepts in the case of logistic regression. Nevertheless, a number of options are available to modelers for estimating fit and confidence for these models, and we list a few of these below: Pseudo-\\(R^2\\) measures are attempts to estimate the amount of variance in the outcome that is explained by the fitted model, analogous to the \\(R^2\\) in linear regression. There are numerous variants of pseudo-\\(R^2\\) with some of the most common listed here. McFadden’s \\(R^2\\) works by comparing the likelihood function of the fitted model with that of a random model and using this to estimate the explained variance in the outcome. Cox and Snell’s \\(R^2\\) works by applying a ‘sum of squares’ analogy to the likelihood functions to align more closely with the precise methodology for calculating \\(R^2\\) in linear regression. However, this usually means that the maximum value is less than 1, and in certain circumstances substantially less than 1, which can be problematic and unintuitive for an \\(R^2\\) Nagelkerke’s \\(R^2\\) resolves the issue with the upper bound for Cox and Snell by dividing Cox and Snell’s \\(R^2\\) by its upper bound. This restores an intuitive scale with a maximum of 1, but is considered somewhat arbitrary with limited theoretical foundation. Tjur’s \\(R^2\\) is a more recent and simpler concept. It is defined as simply the absolute difference between the predicted probabilities of the positive observations and those of the negative observations. Standard modeling functions generally do not offer the calculation of pseudo-\\(R^2\\) as standard, but numerous methods are available for their calculation. For example: library(DescTools) DescTools::PseudoR2(simpler_model, which = c(&quot;McFadden&quot;, &quot;CoxSnell&quot;, &quot;Nagelkerke&quot;, &quot;Tjur&quot;)) ## McFadden CoxSnell Nagelkerke Tjur ## 0.8520759 0.6576490 0.9187858 0.8784834 We see that the Cox and Snell variant is notably lower than the other estimates, which confirms the known issues with its upper bound. However, the other estimates are reasonably aligned and suggest a strong fit. Goodness of fit tests for logistic regression models compare the predictions to the observed outcome and test the hypothesis that they are similar. This means that, unlike in linear regression, a low p-value indicates a poor fit. One commonly used method is the Hosmer-Lemeshow test, which divides the observations into a number of groups (usually ten) according to their fitted probabilities, calculates the proportion of each group that is positive and then compares this to the expected proportions based on the model prediction using a Chi-squared test. However, this method has limitations. It is particularly problematic for situations where there a low sample size and can return highly varied results based on the number of groups used. It is therefore recommended to use a range of goodness of fit tests, and not reply on any one specific approach. In R, the LogisticDx package offers a range of diagnostic tools for logistic regression models, and is recommended for exploration. Here is an example for assessing goodness of fit. library(LogisticDx) # get range of goodness of fit diagnostics simpler_model_diagnostics &lt;- LogisticDx::gof(simpler_model, plotROC = FALSE) # returns a list names(simpler_model_diagnostics) ## [1] &quot;ct&quot; &quot;chiSq&quot; &quot;ctHL&quot; &quot;gof&quot; &quot;R2&quot; &quot;auc&quot; # in our case we are interested in goodness of fit statistics simpler_model_diagnostics$gof ## test stat val df pVal ## 1: HL chiSq 3.44576058 8 0.903358158 ## 2: mHL F 2.74709957 8 0.005971045 ## 3: OsRo Z -0.02415249 NA 0.980730971 ## 4: SstPgeq0.5 Z 0.88656856 NA 0.375311227 ## 5: SstPl0.5 Z 0.96819352 NA 0.332947728 ## 6: SstBoth chiSq 1.72340251 2 0.422442787 ## 7: SllPgeq0.5 chiSq 1.85473814 1 0.173233325 ## 8: SllPl0.5 chiSq 0.68570870 1 0.407627859 ## 9: SllBoth chiSq 1.86640617 2 0.393291943 This confirms that almost all tests, including the Hosmer-Lemeshow test which is the first in the list, suggest a fit for our model. Various measures of predictive accuracy can also be used to assess a binomial logistic regression model in a predictive context, such as precision, recall and ROC-curve analysis. These are particularly suited for implementations of logistic regression models as predictive classifiers in a Machine Learning context, a topic which is outside the scope of this book. However, a recommended source for a deeper treatment of goodness of fit tests for logistic regression models is David W. Hosmer Jr. (2013). 3.3.3 Model parsimony We saw that in both our linear regression and our logistic regression approach, we decided to drop variables from our model when we determined that they had no significant effect on the outcome. The principle of Occam’s Razor states that - all else being equal - the simplest explanation is the best. In this sense, a model that contains information that does not contribute to it primary objective is more complex than it needs to be. Such a model increases the communication burden in explaining its results to others, with no notable analytic benefit in return. Parsimony describes the concept of being careful with resources or with information. A model could be described as more parsimonious if it can achieve the same (or close to the same) fit with a smaller number of inputs. The Aikake Information Criterion or AIC is a measure of model parsimony that is computed for log-likelihood models like logistic regression models, with a lower AIC indicating a more parsimonious model. AIC is often calculated as standard in summary reports of logistic regression models, but can also be calculated independently. Let’s compare the different iterations of our model in this chapter using AIC. # Sales only model AIC(sales_model) ## [1] 76.49508 # sales and customer rating model AIC(simpler_model) ## [1] 71.13145 # model with all inputs AIC(full_model) ## [1] 76.37433 We can see that the model which contains only our two significant inputs - sales and customer rating - is determined to be the most parsimonious model according to the AIC. Note that the AIC should not be used to interpret model quality or confidence - it is possible that the lowest AIC might be telling you which model is the best of a bad bunch. Model parsimony becomes a substantial concern when there is a large number of input variables. As a general rule, the more input variables there are in a model the greater the chance that the model will be difficult to interpet clearly, and the greater the risk of measurement problems such as multicollinearity. Analysts who are eager to please their customers, clients, professors or bosses can easily be tempted to think up new potential inputs to their model, often derived mathematically from measures that are already inputs in the model. Before long the model is too complex and in extreme cases there are more inputs than there are observations. The primary way to to manage model complexity is to exercise caution in selecting model inputs. When large numbers of inputs are unavoidable, coefficient regularization methods such as LASSO regression can help with model parsimony. We will look at this briefly in a later chapter. 3.4 Other considerations in binomial logistic regression Many of the principles covered in the previous chapter on linear regression are equally important in logistic regression. For example, input variables should be managed in a similar way. Confidence intervals can be calculated for coefficients and for predictions. Collinearity and multicollinearity should be of concern. Interaction of input variables can be modeled. For the most part, analysts should be aware of the fundamental transformations which take place on the outcome \\(y\\) during logistic regression when they consider some of these issues (another reason to ensure that the mathematics covered earlier in this chapter is well understood). For example, while coefficients in linear regression have a direct additive impact on \\(y\\), in logistic regression they have a direct additive impact on the log odds of \\(y\\), or alternatively their exponents have a direct multiplicative impact on the odds of \\(y\\). Therefore, confidence intervals of the coefficients can be interpreted as a range of additive factors for the log odds, or the exponents can be considered a range of multiplicative factors for the odds. Similarly, multicollinear variables risk inflating log odds additively, which risk inflating the odds multiplicatively. Because of the binary nature of our outcome variable, the residuals of a logistic regression model have limited direct application to the problem being studied. In practical contexts the residuals of logistic regression models are rarely examined but can be useful in identifying outliers or particularly influential observations and in assessing goodness of fit, as in 3.3.2. When residuals are examined, they need to be transformed in order to be analyzed appropriately. For example, the Pearson residual is a standardized form of residual from logistic regression which can be expected to have a normal distribution over large enough samples. We can see in Figure 3.7 that this is the case for our simpler_model, but that there are a small number of substantial underestimates in our model. A good source of further learning on diagnostics of logistic regression models is Menard (2010). d &lt;- density(residuals(simpler_model, &quot;pearson&quot;)) plot(d, main= &quot;&quot;) Figure 3.7: Distribution of Pearson residuals in simpler_model 3.5 Learning exercises 3.5.1 Discussion questions Draw the shape of a logistic (sigmoid) function. Describe the three population growth phases it was originally intended to model. Explain why the logistic function is useful to statisticians in modeling. In the formula for the logistic function in Section 3.1.1, what might be a common value for \\(L\\) in probabilistic applications? Why? What types of problems are suitable for logistic regression modeling? Can you think of some modeling scenarios in your work or studies that could use a logistic regression approach? Explain the concept of odds. How do odds differ from probability? How do odds change as probability increases? Complete the following: If an event has a 1% probability of occurring, a 10% increase in odds results in an almost __% increase in probability If an event has a 99% probability of occurring, a 10% increase in odds results in an almost __% increase in probability Describe how the coefficients of a logistic regression model affect the fitted outcome. If \\(\\beta\\) is a coefficient estimate, how is the odds ratio associated with \\(\\beta\\) calculated and what does it mean? What are some of the options for determining how well a regression model fits? Describe the concept of model parsimony. What measure is commonly used to determine the most parsimonious logistic regression model? 3.5.2 Data exercise References "]
]
